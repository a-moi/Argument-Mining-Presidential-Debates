{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Setting 2: Features + rbf SVM \n",
    "### Task 2: Classification Claims vs Premises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing necessary packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mariap/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mariap/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import collections\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag, word_tokenize, RegexpParser\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../../../data/sentence_db_candidate.csv'\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple preprocessing of sentences with lowercasing and punctuation removal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join([i for i in sentence if i not in string.punctuation])\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Speech'] = df['Speech'].apply(preproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below we are turning labels \"Claim\" and \"Premise\" into machine-readable classes: 1 and 0 respectively**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps = ['Claim', 'Premise']\n",
    "df = df.loc[(df['Component'].isin(comps))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "\n",
    "for s in df.Component:\n",
    "    if s == 'Claim':\n",
    "        classes.append(1.0)\n",
    "    else:\n",
    "        classes.append(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Annotation'] = classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22280, 19)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Speech', 'Annotation', 'Set']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speech</th>\n",
       "      <th>Annotation</th>\n",
       "      <th>Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and after 911 it became clear that we had to d...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and we also then finally had to stand up democ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>what we did in iraq was exactly the right thin...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>if i had it to recommend all over again i woul...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>the world is far safer today because saddam hu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Speech  Annotation    Set\n",
       "3   and after 911 it became clear that we had to d...         1.0  TRAIN\n",
       "4   and we also then finally had to stand up democ...         0.0  TRAIN\n",
       "9   what we did in iraq was exactly the right thin...         1.0  TRAIN\n",
       "10  if i had it to recommend all over again i woul...         0.0  TRAIN\n",
       "11  the world is far safer today because saddam hu...         0.0  TRAIN"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    11964\n",
       "0.0    10316\n",
       "Name: Annotation, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Annotation'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we are starting feature engineering. The functions are designed to take the full dataframe and a column with sentences and to output a dataframe with a newly added feature(s). Please refer to `linguistic_features.py` file to see/copy the documented feature functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Feature: Part-of-Speech (adverbs and adjectives)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_count(lst):\n",
    "    \n",
    "    \"\"\"\n",
    "    :function: count the elements of a list -- the number of words with a respective POS or NER labels in a sentence. \n",
    "    :input: lst: list of tuples, where tuple has two elements -- a word and its POS or NER label\n",
    "    :return: lst_count: list of dictionaries, where\n",
    "    the dictionary consists of keys -- the elements are words and their POS or NER labels\n",
    "    and values -- how many times each word and its POS or NER label occurs\n",
    "    If a sentence has no POS or NER labels, return an empty list \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    dic_counter = collections.Counter()\n",
    "    \n",
    "    for x in lst:\n",
    "        dic_counter[x] += 1\n",
    "    \n",
    "    dic_counter = collections.OrderedDict( \n",
    "                     sorted(dic_counter.items(), \n",
    "                     key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    lst_count = [{key:value} for key,value in dic_counter.items()]\n",
    "    \n",
    "    return lst_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_tag(lst_dics_tuples, tag):\n",
    "    \n",
    "    \"\"\"\n",
    "    :function: new column for each POS or NER tag category \n",
    "    :input: lst_dics_tuples: list of dictionaries with tuples \n",
    "            tag: POS or NER label from a list\n",
    "    :return: tag: new column for each POS or NER label with their counts\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(lst_dics_tuples) > 0:\n",
    "        tag_type = []\n",
    "        \n",
    "        for dic_tuples in lst_dics_tuples:\n",
    "            for tuple in dic_tuples:\n",
    "                type, n = tuple[1], dic_tuples[tuple]\n",
    "                tag_type = tag_type + [type]*n\n",
    "                dic_counter = collections.Counter()\n",
    "                for x in tag_type:\n",
    "                    dic_counter[x] += 1\n",
    "        return dic_counter[tag]\n",
    "    \n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features (df, speech_sents):\n",
    "    \n",
    "    \"\"\"\n",
    "    :function: add two new columns with two POS: adjectives and adverbs, and their counts per sentence.\n",
    "    Two helper functions -- list_count, column_tag -- are needed \n",
    "    :input: df: entire DataFrame\n",
    "            speech_sents: Series of sentences in DataFrame\n",
    "    :return: df: new DataFrame with two new features\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    df['pos'] = speech_sents.apply(lambda x: [(tag.text, tag.pos_) \n",
    "                                for tag in nlp(x)])\n",
    "    \n",
    "    df['pos'] = df['pos'].apply(lambda x: list_count(x))\n",
    "    \n",
    "    #extract features\n",
    "    tags_set = []\n",
    "\n",
    "    for lst in df['pos'].tolist():\n",
    "        for dic in lst:\n",
    "            for k in dic.keys():\n",
    "                tags_set.append(k[1])\n",
    "            \n",
    "    tags_set = list(set(tags_set))\n",
    "\n",
    "    for feature in tags_set:\n",
    "        df['pos_' + feature] = df['pos'].apply(lambda x: column_tag(x, feature))\n",
    "        \n",
    "    # keeping only adverbs and adjectives and dropping other pos\n",
    "    for feature in df.columns:\n",
    "        if feature != 'pos_ADV' and feature != 'pos_ADJ' and 'pos' in feature:\n",
    "            df = df.drop(feature, axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pos_features(df, df['Speech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Feature: Named Entity Recognition labels*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_features(df, speech_sents):\n",
    "    \n",
    "    \"\"\"\n",
    "    :function: add several new columns with NER labels, and their counts per sentence.\n",
    "    Two helper functions -- list_count, column_tag -- are needed \n",
    "    :input: df: entire DataFrame\n",
    "            speech_sents: Series of sentences in DataFrame\n",
    "    :return: df: new DataFrame with new features for each NER label\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    df['ner'] = speech_sents.apply(lambda x: [(tag.text, tag.label_) \n",
    "                                for tag in nlp(x).ents])\n",
    "    # count tags\n",
    "    df['ner'] = df['ner'].apply(lambda x: list_count(x))\n",
    "    \n",
    "    # extract features\n",
    "    tags_set = []\n",
    "\n",
    "    for lst in df['ner'].tolist():\n",
    "        for dic in lst:\n",
    "            for k in dic.keys():\n",
    "                tags_set.append(k[1])\n",
    "            \n",
    "    tags_set = list(set(tags_set))\n",
    "\n",
    "    for feature in tags_set:\n",
    "        df['ner_' + feature] = df['ner'].apply(lambda x: column_tag(x, feature))\n",
    "        \n",
    "    df = df.drop(['ner'], axis=1)\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ner_features(df, df['Speech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Feature: Verbs' tenses and modals*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbs_features (df, speech_sents):\n",
    "    \n",
    "    \"\"\"\n",
    "    :function: add several new columns with features for verb tenses and the presence of modal verbs, \n",
    "    and their counts per sentence.\n",
    "    Two helper functions -- list_count, column_tag -- are needed \n",
    "    :input: df: entire DataFrame\n",
    "            speech_sents: Series of sentences in DataFrame\n",
    "    :return: df: new DataFrame with features for each verb tense and for modal verbs\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    df['verb_tag'] = speech_sents.apply(lambda x: [(tag.text, tag.tag_) \n",
    "                                for tag in nlp(x)])\n",
    "    \n",
    "    df['verb_tag'] = df['verb_tag'].apply(lambda x: list_count(x))\n",
    "    \n",
    "    #extract features\n",
    "    verbs_set = []\n",
    "\n",
    "    for lst in df['verb_tag'].tolist():\n",
    "        for dic in lst:\n",
    "            for k in dic.keys():\n",
    "                verbs_set.append(k[1])\n",
    "            \n",
    "    verbs_set = list(set(verbs_set))\n",
    "\n",
    "    for feature in verbs_set:\n",
    "        df['verb_tag_' + feature] = df['verb_tag'].apply(lambda x: column_tag(x, feature))\n",
    "    \n",
    "    #out of all detailed POS tags, keeping only verbs-related ones \n",
    "    for f in df.columns:\n",
    "        if f != 'verb_tag_VB' and f != 'verb_tag_VBZ' and f != 'verb_tag_VBP' and f != 'verb_tag_VBD' and f != 'verb_tag_VBN' and f != 'verb_tag_VBG' and f != 'verb_tag_MD' and 'verb_tag' in f:\n",
    "            df = df.drop(f, axis=1)\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = verbs_features(df, df['Speech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Feature: Syntactic features (the number of productions, the number of Verbal Phrases groups, the depth of a sentence tree)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syntactic_features(df, speech_sents):\n",
    "        \n",
    "    \"\"\"\n",
    "    :function: add syntactic features -- 1) the number of productions, 2) the number of VP groups per sentence, \n",
    "    and 3) the depth of a sentence tree \n",
    "    :input: df: entire DataFrame\n",
    "            speech_sents: Series of sentences in DataFrame\n",
    "    :return: df: new DataFrame with three syntactic features \n",
    "\n",
    "    \"\"\"\n",
    "  \n",
    "    a, b , c, d, e = [], [], [], [], []\n",
    "    for x, y in enumerate(speech_sents):\n",
    "        tagged = pos_tag(word_tokenize(y))\n",
    "        chunker = RegexpParser(r\"\"\"\n",
    "            NBAR:\n",
    "            {<NN.*|JJ>*<NN.*>}  \n",
    "            VP:\n",
    "            {<V.*>}  \n",
    "            NP:\n",
    "            {<NBAR>}\n",
    "            {<NBAR><IN><NBAR>}  \n",
    "        \"\"\")\n",
    "    \n",
    "        a.append(chunker.parse(tagged))\n",
    "        b.append(len(chunker.parse(tagged).productions()))\n",
    "        e.append(chunker.parse(tagged).productions())\n",
    "        c.append(chunker.parse(tagged).height())\n",
    "\n",
    "    df.loc[:, 'Speech_parsed'] = a\n",
    "    df.loc[:, 'Productions_count'] = b\n",
    "    df.loc[:, 'Tree_depth'] = c\n",
    "  \n",
    "\n",
    "    for i in e:\n",
    "        vp = []\n",
    "        for u in i:\n",
    "            if str(u).startswith('VP'):\n",
    "                vp.append(u)\n",
    "        d.append(len(vp))\n",
    "  \n",
    "    df.loc[:, 'VP_count'] = d\n",
    "    \n",
    "    df = df.drop(['Speech_parsed'], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariap/anaconda3/lib/python3.7/site-packages/pandas/core/dtypes/missing.py:517: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  val = np.array(val, copy=False)\n",
      "/Users/mariap/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3162: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return asarray(a).ndim\n"
     ]
    }
   ],
   "source": [
    "df = syntactic_features(df, df['Speech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Feature: Sentiment of a sentence*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment (df, speech_sents): \n",
    "    \n",
    "    \n",
    "    \"\"\" \n",
    "    :function: add a feature with a sentiment score for each sentence \n",
    "    :input: df: entire DataFrame\n",
    "            speech_sents: Series of sentences in DataFrame\n",
    "    :return: df: DataFrame with a new feature Sentiment\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    senti = []\n",
    "    \n",
    "    for sent in speech_sents:\n",
    "        vs = analyzer.polarity_scores(sent)\n",
    "        senti.append([list(vs.values())[3]])\n",
    "    \n",
    "    senti_arr = np.array(senti)\n",
    "    df['Sentiment'] = senti_arr\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_sentiment(df, df['Speech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Feature: Discourse connectives*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_connectives (df, speech_sents):\n",
    "\n",
    "    \"\"\" \n",
    "    :function: add a boolean feature based on presence/absence of a claim connective from the pre-defined list \n",
    "    :input: df: entire DataFrame\n",
    "            speech_sents: Series of sentences in DataFrame\n",
    "    :return: df: DataFrame with a new feature Claim_Connective\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    connectives = ['so that', 'as a result', 'therefore', 'thus', 'thereby', 'in the end', 'hence', 'accordingly', 'in this way', 'because', 'now that', 'insofar as', 'given that', 'in response to', 'consequently', 'as a consequence']\n",
    "    lst = []\n",
    "    \n",
    "    for sent in speech_sents:\n",
    "        if any(w in sent for w in connectives):\n",
    "            lst.append(1)\n",
    "        else:\n",
    "            lst.append(0)\n",
    "    df['Claim_Connective'] = lst\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_connectives(df, df['Speech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Feature: Personal pronouns*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_personal(df, speech_sents):\n",
    "    \"\"\"\n",
    "    :function: add two boolean features based on the presence/absence of any pronoun from two given lists.\n",
    "    :input: df: entire DataFrame\n",
    "            speech_sents: Series of sentences in DataFrame\n",
    "    :return: df: DataFrame with two new features Pronoun_Singular and Pronoun_Plural\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    singular = [' i ', ' me ', ' my ', ' myself ', ' mine ']\n",
    "    plural = [' we ', ' our ', ' ours ', ' ourselves ']\n",
    "    lst_sing = []\n",
    "    lst_plur = []\n",
    "\n",
    "    for sent in speech_sents:\n",
    "        if any(w in sent for w in singular):\n",
    "            lst_sing.append(1)\n",
    "        else:\n",
    "            lst_sing.append(0)\n",
    "    df['Pronoun_Singular'] = lst_sing\n",
    "\n",
    "    for sent in speech_sents:\n",
    "        if any(w in sent for w in plural):\n",
    "            lst_plur.append(1)\n",
    "        else:\n",
    "            lst_plur.append(0)\n",
    "    df['Pronoun_Plural'] = lst_plur\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_personal(df, df['Speech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf uni-, bi- and tri-grams and training preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting the data into three sets. Our sets will be identical to those of authors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['Set'] == 'TRAIN']\n",
    "df_val = df[df['Set'] == 'VALIDATION']\n",
    "df_test = df[df['Set'] == 'TEST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10464, 37)\n",
      "(5241, 37)\n",
      "(6575, 37)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(10464, 37)\n",
    "(5241, 37)\n",
    "(6575, 37)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['Set'], axis=1)\n",
    "df_val = df_val.drop(['Set'], axis=1)\n",
    "df_test = df_test.drop(['Set'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separating features set and target variable set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(['Annotation'], axis=1)\n",
    "y_train = df_train.Annotation\n",
    "\n",
    "X_val = df_val.drop(['Annotation'], axis=1)\n",
    "y_val = df_val.Annotation\n",
    "\n",
    "X_test = df_test.drop(['Annotation'], axis=1)\n",
    "y_test = df_test.Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initializing tf-idf feature matrix. Fitting and transforming sentences on a train set and only transforming on a validation and test sets. Here we are using unigrams as well as bigrams and trigrams, set by the parameter `ngram_range`. Following the authors' practice, we will be using only the top occuring unigrams from the vocabulary, 10.000 n-grams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,3))\n",
    "\n",
    "#tf-idf\n",
    "train_vecs =  vectorizer.fit_transform(X_train['Speech'])\n",
    "val_tfidf = vectorizer.transform(X_val['Speech'])\n",
    "test_vecs = vectorizer.transform(X_test['Speech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformations of tf-idf matrix for further concatenation with other features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = vectorizer.get_feature_names()\n",
    "dense = train_vecs.todense()\n",
    "denselist = dense.tolist()\n",
    "fe = pd.DataFrame(denselist, columns = names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(['Speech'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.hstack([X_train, fe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing preparations\n",
    "names = vectorizer.get_feature_names()\n",
    "dense = test_vecs.todense()\n",
    "denselist = dense.tolist()\n",
    "fe = pd.DataFrame(denselist, columns = names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.drop(['Speech'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = np.hstack([X_test, fe])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Replication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First of all, repeating the authors' setting. Kernel is `rbf`, penalty parameter `C=10`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, random_state=42)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC(kernel='rbf', C=10, random_state=42)\n",
    "svm.fit(train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_svm = svm.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**class 1 stands for CLAIM, class 0 stands for PREMISE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0      0.694     0.458     0.552      3214\n",
      "     class 1      0.609     0.806     0.694      3361\n",
      "\n",
      "    accuracy                          0.636      6575\n",
      "   macro avg      0.651     0.632     0.623      6575\n",
      "weighted avg      0.650     0.636     0.624      6575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_test, y_pred_test_svm, target_names=target_names, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                      precision recall  f1-score   support\n",
    "\n",
    "     class 0          0.694     0.458     0.552      3214\n",
    "     class 1          0.609     0.806     0.694      3361\n",
    "\n",
    "    accuracy                              0.636      6575\n",
    "    macro avg         0.651     0.632     0.623      6575\n",
    "    weighted avg      0.650     0.636     0.624      6575"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our results are close to authors', but not as high. We perform hyperparameter tuning to improve the scores.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation set preparation\n",
    "names = vectorizer.get_feature_names()\n",
    "dense = val_tfidf.todense()\n",
    "denselist = dense.tolist()\n",
    "fe = pd.DataFrame(denselist, columns = names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_val.drop(['Speech'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features = np.hstack([X_val, fe])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First, initializing the parameters grid. We are tuning parameters `C` and `kernel`, because linear kernel can be potentially good as seen from task 1, same experimental setting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [1, 10], 'kernel': ['linear','rbf']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] C=1, kernel=linear ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................... C=1, kernel=linear, total= 4.6min\n",
      "[CV] C=1, kernel=linear ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................... C=1, kernel=linear, total= 4.6min\n",
      "[CV] C=1, kernel=linear ..............................................\n",
      "[CV] ............................... C=1, kernel=linear, total= 4.6min\n",
      "[CV] C=1, kernel=linear ..............................................\n",
      "[CV] ............................... C=1, kernel=linear, total= 4.7min\n",
      "[CV] C=1, kernel=linear ..............................................\n",
      "[CV] ............................... C=1, kernel=linear, total= 4.4min\n",
      "[CV] C=1, kernel=rbf .................................................\n",
      "[CV] .................................. C=1, kernel=rbf, total= 4.9min\n",
      "[CV] C=1, kernel=rbf .................................................\n",
      "[CV] .................................. C=1, kernel=rbf, total= 4.8min\n",
      "[CV] C=1, kernel=rbf .................................................\n",
      "[CV] .................................. C=1, kernel=rbf, total= 5.0min\n",
      "[CV] C=1, kernel=rbf .................................................\n",
      "[CV] .................................. C=1, kernel=rbf, total= 4.7min\n",
      "[CV] C=1, kernel=rbf .................................................\n",
      "[CV] .................................. C=1, kernel=rbf, total= 5.0min\n",
      "[CV] C=10, kernel=linear .............................................\n",
      "[CV] .............................. C=10, kernel=linear, total= 5.8min\n",
      "[CV] C=10, kernel=linear .............................................\n",
      "[CV] .............................. C=10, kernel=linear, total= 6.6min\n",
      "[CV] C=10, kernel=linear .............................................\n",
      "[CV] .............................. C=10, kernel=linear, total= 8.0min\n",
      "[CV] C=10, kernel=linear .............................................\n",
      "[CV] .............................. C=10, kernel=linear, total= 7.0min\n",
      "[CV] C=10, kernel=linear .............................................\n",
      "[CV] .............................. C=10, kernel=linear, total= 6.4min\n",
      "[CV] C=10, kernel=rbf ................................................\n",
      "[CV] ................................. C=10, kernel=rbf, total= 4.7min\n",
      "[CV] C=10, kernel=rbf ................................................\n",
      "[CV] ................................. C=10, kernel=rbf, total= 4.7min\n",
      "[CV] C=10, kernel=rbf ................................................\n",
      "[CV] ................................. C=10, kernel=rbf, total= 4.6min\n",
      "[CV] C=10, kernel=rbf ................................................\n",
      "[CV] ................................. C=10, kernel=rbf, total= 4.5min\n",
      "[CV] C=10, kernel=rbf ................................................\n",
      "[CV] ................................. C=10, kernel=rbf, total= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed: 104.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': [1, 10], 'kernel': ['linear', 'rbf']}, verbose=2)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=2)\n",
    "grid.fit(val_features, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`'C': 10, 'kernel': 'rbf'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion: Hyperparameter tuning is suggesting that the original parameter setting is the best. Upon the results with original parameters, we can conclude that our results are rather close to those of authors, with average F1-score for both classes being 0.624, the authors' - 0.651**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
