{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-shepherd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from tqdm import tqdm\n",
    "from nltk import tokenize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Bidirectional, Embedding, Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from gensim.models import FastText\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-parent",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'sentence_db_candidate.csv'\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-steam",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join([i for i in sentence if i not in string.punctuation])\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-canada",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Speech'] = df['Speech'].apply(preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = ['Claim', 'Premise']\n",
    "df = df.loc[(df['Component'].isin(valid))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turning labels into two classes \n",
    "classes = []\n",
    "\n",
    "for s in df.Component:\n",
    "    if s == 'Claim':\n",
    "        classes.append(1.0)\n",
    "    else:\n",
    "        classes.append(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-liberal",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Annotation'] = classes\n",
    "df.Annotation.value_counts()\n",
    "df = df[['Speech', 'Annotation', 'Set']]\n",
    "\n",
    "max_seq_len = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['Set'] == 'TRAIN']\n",
    "df_val = df[df['Set'] == 'VALIDATION']\n",
    "df_test = df[df['Set'] == 'TEST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-entry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#task 1, compile all sentences(for feature engineering) and corresponding labels, 1 for containing argument component\n",
    "all_sentences = df.iloc[:, 0].tolist()\n",
    "all_sentences_train = df_train.iloc[:, 0].tolist()\n",
    "all_sentences_test = df_test.iloc[:, 0].tolist()\n",
    "\n",
    "all_labels_train = df_train.iloc[:, 1].tolist()\n",
    "all_labels_test = df_test.iloc[:, 1].tolist()\n",
    "\n",
    "print(all_sentences_train[0:5])\n",
    "print(all_labels_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "FT = \"fasttext/wiki-news-300d-1M.vec\"\n",
    "fasttext = KeyedVectors.load_word2vec_format(FT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-jungle",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sent_train_tokenized = []\n",
    "all_sent_test_tokenized = []\n",
    "all_sent_tokenized = []\n",
    "longest_word_len = []\n",
    "for i in range (len(all_sentences)):\n",
    "    all_sent_tokenized.append(word_tokenize(all_sentences[i]))\n",
    "\n",
    "for i in range (len(all_sentences_train)):\n",
    "    all_sent_train_tokenized.append(word_tokenize(all_sentences_train[i]))\n",
    "    \n",
    "for i in range (len(all_sentences_test)):\n",
    "    all_sent_test_tokenized.append(word_tokenize(all_sentences_test[i]))\n",
    "    \n",
    "#print(\"longest sentence: \", max(all_sent_tokenized,key=len))\n",
    "#print(\"longest sentence length: \", len(max(all_sent_tokenized,key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 149"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-surname",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter()\n",
    "\n",
    "for sent in tqdm(all_sent_tokenized):\n",
    "    vocab.update(sent)\n",
    "    \n",
    "unique_words = len(fasttext)\n",
    "word_index = {t[0]: i+1 for i,t in enumerate(vocab.most_common(unique_words))}\n",
    "\n",
    "sequences_train = [[word_index.get(t, 0) for t in sent] for sent in all_sent_train_tokenized]\n",
    "sequences_test = [[word_index.get(t, 0) for t in sent] for sent in all_sent_test_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-coaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pad_sequences(sequences_train, maxlen=max_seq_len, padding=\"pre\", truncating=\"post\")\n",
    "print('Shape of data:', data_train.shape)\n",
    "\n",
    "data_test = pad_sequences(sequences_test, maxlen=max_seq_len, padding=\"pre\", truncating=\"post\")\n",
    "print('Shape of data:', data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-holder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we initialize the matrix with random numbers\n",
    "embedding_matrix = (np.random.rand(unique_words, 300) - 0.5) / 5.0\n",
    "for word, i in word_index.items():\n",
    "    if i >= unique_words:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = fasttext[word]\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('cpu:0'):\n",
    "  embedding_layer = Embedding(len(fasttext), 300, weights = [embedding_matrix] , trainable=False)\n",
    "  embedding_layer.build((len(fasttext), 300))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Bidirectional(LSTM(200, return_sequences=False, activation=\"sigmoid\"), input_shape=(300, 1)))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"AUC\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.asarray(data_train)\n",
    "all_labels_train = np.asarray(all_labels_train)\n",
    "model.fit(x=data_train, y=all_labels_train, epochs=1,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(data_test)\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i][0] > 0.5:\n",
    "        y_pred[i][0] = 1\n",
    "    else:\n",
    "        y_pred[i][0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-cisco",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['Premise', 'Claim']\n",
    "print(classification_report(all_labels_test, y_pred, target_names=target_names, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-calendar",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
