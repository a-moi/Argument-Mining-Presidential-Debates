{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "affecting-superior",
   "metadata": {},
   "source": [
    "<h2>Load the data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "missing-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "auburn-mixer",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_files = []\n",
    "for name in glob.glob('ElecDeb60To16/*.txt'):\n",
    "    name = name.replace('\\\\','/')\n",
    "    txt_files.append(name)\n",
    "\n",
    "ann_files = []\n",
    "for name in glob.glob('ElecDeb60To16/*.ann'):\n",
    "    name = name.replace('\\\\','/')\n",
    "    ann_files.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "derived-jewel",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_all = []\n",
    "ind_all = []\n",
    "\n",
    "for file in txt_files:\n",
    "    sent_list = []\n",
    "    ind_list = []\n",
    "    prev_ind = 0\n",
    "    \n",
    "    txt_file = open(file)\n",
    "    text = txt_file.read()\n",
    "    txt_file.close()\n",
    "    \n",
    "    \n",
    "    with open(file) as infile:\n",
    "        for line in infile:\n",
    "            line = re.sub(r'(\\([A-Z]+\\)_[A-Z]+(\\_[A-Z]+)?: )','',line)\n",
    "            line = re.sub(r'([A-Z]+\\_[A-Z]+\\_[A-Z]+: )','',line)\n",
    "            line = re.sub(r'([A-Z]+\\_[A-Z]+: )','',line)\n",
    "            line = re.sub(r'(\\_[A-Z]+: )','',line)\n",
    "            line = re.sub(r'([A-Z]+: )','',line)\n",
    "            line = re.sub(r'\\[[a-zA-Z0-9;,\\s\\\"\\'\\.\\S]*\\]*','',line)\n",
    "                  \n",
    "            for sent in tokenize.sent_tokenize(line):  \n",
    "                if not sent_list:\n",
    "                    ind = text.find(sent,0,len(text)) \n",
    "                else:\n",
    "                    ind = text.find(sent,prev_ind,len(text))\n",
    "                prev_ind = ind\n",
    "                sent_list.append([sent,ind])\n",
    "                ind_list.append(ind)\n",
    "                \n",
    "    ind_all.append(np.array(ind_list))\n",
    "    sent_all.append(sent_list)\n",
    "\n",
    "#Up to this point we get a sent_all containing every debate (42 total) tokenized into sentences.\n",
    "#sent_all = [sent_list, sent_list, sent_list,...]\n",
    "#sent_list = [[sent1, index],[sent2, index],[sent3,index],...]\n",
    "#ind_all = [ind_list, ind_list, ind_list,...]\n",
    "#ind_list = [ind, ind, ind,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "southeast-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_all = []\n",
    "\n",
    "for file in ann_files:\n",
    "    ann_list = []\n",
    "    \n",
    "    with open(file) as infile:\n",
    "        for line in infile:\n",
    "            entry = line.split(\"\\t\")\n",
    "            ann_list.append(entry)\n",
    "    \n",
    "    ann_all.append(ann_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "unexpected-growing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#following part creates labels for task 1 (whether a sentence contains an argument component)\n",
    "\n",
    "labels_arg_all = []\n",
    "\n",
    "for i in range(len(ind_all)):\n",
    "    labels_arg_arr = np.empty([len(ind_all[i]),2])\n",
    "    sent_start_ind_list = []\n",
    "\n",
    "    #column 0 of labels_arg are the starting indices of the senteces \n",
    "    for j in range(len(ind_all[i])):\n",
    "        labels_arg_arr[j,0] = ind_all[i][j]\n",
    "        \n",
    "    #loop through each annotation entry, get the corresponding sentence starting index \n",
    "    for j in range(len(ann_all[i])):\n",
    "        phrase_start = re.search(r'[0-9]+\\s',ann_all[i][j][1]).group(0)\n",
    "        sent_start_ind_list.append(ind_all[i][ind_all[i] <= int(phrase_start)].max())\n",
    "        \n",
    "        phrase_start = re.search(r';[0-9]+\\s',ann_all[i][j][1])\n",
    "        if phrase_start != None:\n",
    "            phrase_start = phrase_start.group(0)[1:]\n",
    "            ind2 = ind_all[i][ind_all[i] <= int(phrase_start)].max()\n",
    "            if ind2 not in sent_start_ind_list:\n",
    "                sent_start_ind_list.append(ind2)\n",
    "    \n",
    "    #loop through starting indices for sentences, check if each sentence contains an annotation (labeled as 1)\n",
    "    for j in range(len(ind_all[i])):   \n",
    "        if ind_all[i][j] in sent_start_ind_list:\n",
    "            labels_arg_arr[j,1] = 1\n",
    "        else:\n",
    "            labels_arg_arr[j,1] = 0\n",
    "    \n",
    "    labels_arg_all.append(labels_arg_arr.astype(int))\n",
    "\n",
    "        \n",
    "#ann_all = [ann_list, ann_list, ann_list, ...]\n",
    "#ann_list = [['T1', 'Claim/Premise xxx xxx', '...'], ['T2', 'Claim/Premise xxx xxx', '...'], \n",
    "    #['T3', 'Claim/Premise xxx xxx', '...'], ...]\n",
    "#labels_arg_all = [labels_arg_arr, labels_arg_arr, labesl_arg_arr, ...]\n",
    "#labels_arg_arr = np array of size (len(ind_list),2), no. of rows = no. of sentences in a txt file, \n",
    "    #col 0 = starting index of sentece, col 1 = label (1 for contain claim/premise, 0 for otherwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "retained-vision",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_cp_all = []\n",
    "\n",
    "for i in range(len(labels_arg_all)):\n",
    "    labels_cp_list = []\n",
    "    \n",
    "    for j in range(labels_arg_all[i].shape[0]):\n",
    "        if labels_arg_all[i][j,1] != 0:\n",
    "            labels_cp_list.append(labels_arg_all[i][j,0])\n",
    "    \n",
    "    labels_cp_arr = np.zeros((len(labels_cp_list),3))\n",
    "    labels_cp_out = np.zeros((len(labels_cp_list),2))\n",
    "    \n",
    "    for k in range(len(labels_cp_list)):\n",
    "        labels_cp_arr[k,0] = labels_cp_list[k]\n",
    "\n",
    "        \n",
    "    for j in range(len(ann_all[i])):\n",
    "        #check for claim with only 1 continuous part\n",
    "        phrase_start = re.search(r'c|Claim\\s[0-9]+\\s[0-9]+',ann_all[i][j][1])\n",
    "        if phrase_start != None:\n",
    "            phrase_start = phrase_start.group(0)\n",
    "            labels_cp_arr[np.where(labels_cp_arr[:,0] == ind_all[i][ind_all[i] <= int(phrase_start.split()[1])].max())[0],1] +=\\\n",
    "            int(phrase_start.split()[2]) - int(phrase_start.split()[1])\n",
    "        \n",
    "        #check for claim formed from 2 separate parts\n",
    "        phrase_start = re.search(r'c|Claim[0-9\\s]+;[0-9]+\\s[0-9]+',ann_all[i][j][1])\n",
    "        if phrase_start != None:\n",
    "            phrase_start = phrase_start.group(0)\n",
    "            labels_cp_arr[np.where(labels_cp_arr[:,0] == ind_all[i][ind_all[i] <= int(phrase_start.split(';')[1].split()[0])].max())[0],1] +=\\\n",
    "            int(phrase_start.split(';')[1].split()[1]) - int(phrase_start.split(';')[1].split()[0])\n",
    "        \n",
    "        #check for premise with only 1 continuous part\n",
    "        phrase_start = re.search(r'p|Premise\\s[0-9]+\\s[0-9]+',ann_all[i][j][1])\n",
    "        if phrase_start != None:\n",
    "            phrase_start = phrase_start.group(0)\n",
    "            labels_cp_arr[np.where(labels_cp_arr[:,0] == ind_all[i][ind_all[i] <= int(phrase_start.split()[1])].max())[0],2] +=\\\n",
    "            int(phrase_start.split()[2]) - int(phrase_start.split()[1])\n",
    "        \n",
    "        #check for premise formed from 2 separate parts\n",
    "        phrase_start = re.search(r'p|Premise[0-9\\s]+;[0-9]+\\s[0-9]+',ann_all[i][j][1])\n",
    "        if phrase_start != None:\n",
    "            phrase_start = phrase_start.group(0)\n",
    "            labels_cp_arr[np.where(labels_cp_arr[:,0] == ind_all[i][ind_all[i] <= int(phrase_start.split(';')[1].split()[0])].max())[0],2] +=\\\n",
    "            int(phrase_start.split(';')[1].split()[1]) - int(phrase_start.split(';')[1].split()[0])\n",
    "    \n",
    "    #compare length of claim and premise components in a sentence, and label the sentence according to the longer component\n",
    "    labels_cp_out[:,0] = labels_cp_arr[:,0]\n",
    "    for m in range(labels_cp_arr.shape[0]):\n",
    "        if labels_cp_arr[m,1] >= labels_cp_arr[m,2]:\n",
    "            labels_cp_out[m,1] = 1\n",
    "            \n",
    "    labels_cp_all.append(labels_cp_out)\n",
    "    \n",
    "#labels_cp_all = [labels_cp_out, labels_cp_out, labels_cp_out, ...]\n",
    "#labels_cp_out = np array of size ((len(labels_cp_list),3)), \n",
    "    #no. of rows = no. sentences that contains at least 1 claim or 1 premise,\n",
    "    #col 0 = starting index of such a sentence, col 1 = label (1 for claim, 0 for premise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-citizen",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
