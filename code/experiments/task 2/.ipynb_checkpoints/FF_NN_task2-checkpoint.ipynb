{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "muslim-adjustment",
   "metadata": {},
   "source": [
    "## Experimental Setting 4: Features + Feed Forward Neural Network \n",
    "### Task 2: Classification Claims vs Premises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "absent-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import spacy\n",
    "import collections\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import keras\n",
    "import tensorflow_addons as tfa\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-kelly",
   "metadata": {},
   "source": [
    "1. Load SpaCy model for pos and ner tagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "existing-vehicle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import collections\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-strengthening",
   "metadata": {},
   "source": [
    "2. Import the raw dataset as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "classical-concentrate",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../../../data/sentence_db_candidate.csv'\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "transsexual-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join([i for i in sentence if i not in string.punctuation])\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "coastal-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Speech'] = df['Speech'].apply(preproc)\n",
    "valid = ['Claim', 'Premise']\n",
    "df = df.loc[(df['Component'].isin(valid))]\n",
    "\n",
    "classes = []\n",
    "\n",
    "for s in df.Component:\n",
    "    if s == 'Claim':\n",
    "        classes.append(1.0)\n",
    "    else:\n",
    "        classes.append(0.0)\n",
    "        \n",
    "df['Annotation'] = classes\n",
    "df.Annotation.value_counts()\n",
    "df = df[['Speech', 'Annotation', 'Set']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-merit",
   "metadata": {},
   "source": [
    "3. We extract different kind of features from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "expanded-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_count(lst):\n",
    "    \n",
    "    \"\"\"\n",
    "    :function: count the elements of a list -- the number of words with a respective POS or NER labels in a sentence. \n",
    "    :input: lst: list of tuples, where tuple has two elements -- a word and its POS or NER label\n",
    "    :return: lst_count: list of dictionaries, where\n",
    "    the dictionary consists of keys -- the elements are words and their POS or NER labels\n",
    "    and values -- how many times each word and its POS or NER label occurs\n",
    "    If a sentence has no POS or NER labels, return an empty list \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    dic_counter = collections.Counter()\n",
    "    \n",
    "    for x in lst:\n",
    "        dic_counter[x] += 1\n",
    "    \n",
    "    dic_counter = collections.OrderedDict( \n",
    "                     sorted(dic_counter.items(), \n",
    "                     key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    lst_count = [{key:value} for key,value in dic_counter.items()]\n",
    "    \n",
    "    return lst_count\n",
    "\n",
    "\n",
    "def column_tag(lst_dics_tuples, tag):\n",
    "    \n",
    "    \"\"\"\n",
    "    :function: new column for each POS or NER tag category \n",
    "    :input: lst_dics_tuples: list of dictionaries with tuples \n",
    "            tag: POS or NER label from a list\n",
    "    :return: tag: new column for each POS or NER label with their counts\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(lst_dics_tuples) > 0:\n",
    "        tag_type = []\n",
    "        \n",
    "        for dic_tuples in lst_dics_tuples:\n",
    "            for tuple in dic_tuples:\n",
    "                type, n = tuple[1], dic_tuples[tuple]\n",
    "                tag_type = tag_type + [type]*n\n",
    "                dic_counter = collections.Counter()\n",
    "                for x in tag_type:\n",
    "                    dic_counter[x] += 1\n",
    "        return dic_counter[tag]\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def add_connectives (df, speech_sents):\n",
    "\n",
    "    \"\"\" \n",
    "    :function: add a boolean feature based on presence/absence of a claim connective from the pre-defined list \n",
    "    :input: df: entire DataFrame\n",
    "            speech_sents: Series of sentences in DataFrame\n",
    "    :return: df: DataFrame with a new feature Claim_Connective\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    connectives = ['so that', 'as a result', 'therefore', 'thus', 'thereby', 'in the end', 'hence', 'accordingly', 'in this way', 'because', 'now that', 'insofar as', 'given that', 'in response to', 'consequently', 'as a consequence']\n",
    "    lst = []\n",
    "    \n",
    "    for sent in speech_sents:\n",
    "        if any(w in sent for w in connectives):\n",
    "            lst.append(1)\n",
    "        else:\n",
    "            lst.append(0)\n",
    "    df['Claim_Connective'] = lst\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def pos_features (df, speech_sents):\n",
    "    \n",
    "    \"\"\"\n",
    "    :function: add two new columns with two POS: adjectives and adverbs, and their counts per sentence.\n",
    "    Two helper functions -- list_count, column_tag -- are needed \n",
    "    :input: df: entire DataFrame\n",
    "            speech_sents: Series of sentences in DataFrame\n",
    "    :return: df: new DataFrame with two new features\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    df['pos'] = speech_sents.apply(lambda x: [(tag.text, tag.pos_) \n",
    "                                for tag in nlp(x)])\n",
    "    \n",
    "    df['pos'] = df['pos'].apply(lambda x: list_count(x))\n",
    "    \n",
    "    #extract features\n",
    "    tags_set = []\n",
    "\n",
    "    for lst in df['pos'].tolist():\n",
    "        for dic in lst:\n",
    "            for k in dic.keys():\n",
    "                tags_set.append(k[1])\n",
    "            \n",
    "    tags_set = list(set(tags_set))\n",
    "\n",
    "    for feature in tags_set:\n",
    "        df['pos_' + feature] = df['pos'].apply(lambda x: column_tag(x, feature))\n",
    "        \n",
    "    # keeping only adverbs and adjectives and dropping other pos\n",
    "    for feature in df.columns:\n",
    "        if feature != 'pos_ADV' and feature != 'pos_ADJ' and 'pos' in feature:\n",
    "            df = df.drop(feature, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def ner_features(df, speech_sents):\n",
    "    \n",
    "    \"\"\"\n",
    "    :function: add several new columns with NER labels, and their counts per sentence.\n",
    "    Two helper functions -- list_count, column_tag -- are needed \n",
    "    :input: df: entire DataFrame\n",
    "            speech_sents: Series of sentences in DataFrame\n",
    "    :return: df: new DataFrame with new features for each NER label\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    df['ner'] = speech_sents.apply(lambda x: [(tag.text, tag.label_) \n",
    "                                for tag in nlp(x).ents])\n",
    "    # count tags\n",
    "    df['ner'] = df['ner'].apply(lambda x: list_count(x))\n",
    "    \n",
    "    # extract features\n",
    "    tags_set = []\n",
    "\n",
    "    for lst in df['ner'].tolist():\n",
    "        for dic in lst:\n",
    "            for k in dic.keys():\n",
    "                tags_set.append(k[1])\n",
    "            \n",
    "    tags_set = list(set(tags_set))\n",
    "\n",
    "    for feature in tags_set:\n",
    "        df['ner_' + feature] = df['ner'].apply(lambda x: column_tag(x, feature))\n",
    "        \n",
    "    df = df.drop(['ner'], axis=1)\n",
    "    \n",
    "    return df \n",
    "\n",
    "\n",
    "def verbs_features (df, speech_sents):\n",
    "    \n",
    "    \"\"\"\n",
    "    :function: add several new columns with features for verb tenses and the presence of modal verbs, \n",
    "    and their counts per sentence.\n",
    "    Two helper functions -- list_count, column_tag -- are needed \n",
    "    :input: df: entire DataFrame\n",
    "            speech_sents: Series of sentences in DataFrame\n",
    "    :return: df: new DataFrame with features for each verb tense and for modal verbs\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    df['verb_tag'] = speech_sents.apply(lambda x: [(tag.text, tag.tag_) \n",
    "                                for tag in nlp(x)])\n",
    "    \n",
    "    df['verb_tag'] = df['verb_tag'].apply(lambda x: list_count(x))\n",
    "    \n",
    "    #extract features\n",
    "    verbs_set = []\n",
    "\n",
    "    for lst in df['verb_tag'].tolist():\n",
    "        for dic in lst:\n",
    "            for k in dic.keys():\n",
    "                verbs_set.append(k[1])\n",
    "            \n",
    "    verbs_set = list(set(verbs_set))\n",
    "\n",
    "    for feature in verbs_set:\n",
    "        df['verb_tag_' + feature] = df['verb_tag'].apply(lambda x: column_tag(x, feature))\n",
    "    \n",
    "    #out of all detailed POS tags, keeping only verbs-related ones \n",
    "    for f in df.columns:\n",
    "        if f != 'verb_tag_VB' and f != 'verb_tag_VBZ' and f != 'verb_tag_VBP' and f != 'verb_tag_VBD' and f != 'verb_tag_VBN' and f != 'verb_tag_VBG' and f != 'verb_tag_MD' and 'verb_tag' in f:\n",
    "            df = df.drop(f, axis=1)\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_personal(df, speech_sents):\n",
    "    \"\"\"\n",
    "    :function: add two boolean features based on the presence/absence of any pronoun from two given lists.\n",
    "    :input: df: entire DataFrame\n",
    "            speech_sents: Series of sentences in DataFrame\n",
    "    :return: df: DataFrame with two new features Pronoun_Singular and Pronoun_Plural\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    singular = [' i ', ' me ', ' my ', ' myself ', ' mine ']\n",
    "    plural = [' we ', ' our ', ' ours ', ' ourselves ']\n",
    "    lst_sing = []\n",
    "    lst_plur = []\n",
    "\n",
    "    for sent in speech_sents:\n",
    "        if any(w in sent for w in singular):\n",
    "            lst_sing.append(1)\n",
    "        else:\n",
    "            lst_sing.append(0)\n",
    "    df['Pronoun_Singular'] = lst_sing\n",
    "\n",
    "    for sent in speech_sents:\n",
    "        if any(w in sent for w in plural):\n",
    "            lst_plur.append(1)\n",
    "        else:\n",
    "            lst_plur.append(0)\n",
    "    df['Pronoun_Plural'] = lst_plur\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_sentiment (df, speech_sents): \n",
    "    \n",
    "    \n",
    "    \"\"\" \n",
    "    :function: add a feature with a sentiment score for each sentence \n",
    "    :input: df: entire DataFrame\n",
    "            speech_sents: Series of sentences in DataFrame\n",
    "    :return: df: DataFrame with a new feature Sentiment\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    senti = []\n",
    "    \n",
    "    for sent in speech_sents:\n",
    "        vs = analyzer.polarity_scores(sent)\n",
    "        senti.append([list(vs.values())[3]])\n",
    "    \n",
    "    senti_arr = np.array(senti)\n",
    "    df['Sentiment'] = senti_arr\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "comic-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pos_features(df, df['Speech'])\n",
    "df = ner_features(df, df['Speech'])\n",
    "df = verbs_features(df, df['Speech'])\n",
    "df = add_connectives(df, df['Speech'])\n",
    "df = add_personal(df, df['Speech'])\n",
    "df = add_sentiment(df, df['Speech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "satisfied-surprise",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Peng\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Peng Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag, word_tokenize, RegexpParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "entitled-appearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def syntactic_features(df, speech_sents):\n",
    "        \n",
    "    \"\"\"\n",
    "    :function: add syntactic features -- 1) the number of productions, 2) the number of VP groups per sentence, \n",
    "    and 3) the depth of a sentence tree \n",
    "    :input: df: entire DataFrame\n",
    "            speech_sents: Series of sentences in DataFrame\n",
    "    :return: df: new DataFrame with three syntactic features \n",
    "\n",
    "    \"\"\"\n",
    "  \n",
    "    a, b , c, d, e = [], [], [], [], []\n",
    "    for x, y in enumerate(speech_sents):\n",
    "        tagged = pos_tag(word_tokenize(y))\n",
    "        chunker = RegexpParser(r\"\"\"\n",
    "            NBAR:\n",
    "            {<NN.*|JJ>*<NN.*>}  \n",
    "            VP:\n",
    "            {<V.*>}  \n",
    "            NP:\n",
    "            {<NBAR>}\n",
    "            {<NBAR><IN><NBAR>}  \n",
    "        \"\"\")\n",
    "    \n",
    "        a.append(chunker.parse(tagged))\n",
    "        b.append(len(chunker.parse(tagged).productions()))\n",
    "        e.append(chunker.parse(tagged).productions())\n",
    "        c.append(chunker.parse(tagged).height())\n",
    "\n",
    "    df.loc[:, 'Speech_parsed'] = a\n",
    "    df.loc[:, 'Productions_count'] = b\n",
    "    df.loc[:, 'Tree_depth'] = c\n",
    "  \n",
    "\n",
    "    for i in e:\n",
    "        vp = []\n",
    "        for u in i:\n",
    "            if str(u).startswith('VP'):\n",
    "                vp.append(u)\n",
    "        d.append(len(vp))\n",
    "  \n",
    "    df.loc[:, 'VP_count'] = d\n",
    "    \n",
    "    df = df.drop(['Speech_parsed'], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = syntactic_features(df, df['Speech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-survey",
   "metadata": {},
   "source": [
    "4. Prepare the dataframe for training and testing. Also, include bag-of-words and ngram features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "packed-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting as the authors did \n",
    "df_train = df[df['Set'] == 'TRAIN']\n",
    "df_val = df[df['Set'] == 'VALIDATION']\n",
    "df_test = df[df['Set'] == 'TEST']\n",
    "\n",
    "df_train = df_train.drop(['Set'], axis=1)\n",
    "df_test = df_test.drop(['Set'], axis=1)\n",
    "\n",
    "X_train = df_train.drop(['Annotation'], axis=1)\n",
    "y_train = df_train.Annotation\n",
    "X_test = df_test.drop(['Annotation'], axis=1)\n",
    "y_test = df_test.Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "particular-cambodia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10034\n",
      "(10464, 10034)\n",
      "[[2. 6. 0. ... 0. 0. 0.]\n",
      " [5. 3. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 2. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,3))\n",
    "train_vecs = vectorizer.fit_transform(X_train['Speech'])\n",
    "test_vecs = vectorizer.transform(X_test['Speech'])\n",
    "\n",
    "names = vectorizer.get_feature_names()\n",
    "dense = train_vecs.todense()\n",
    "denselist = dense.tolist()\n",
    "fe = pd.DataFrame(denselist, columns = names)\n",
    "\n",
    "X_train = X_train.drop(['Speech'], axis=1)\n",
    "train_features = np.hstack([X_train, fe])\n",
    "\n",
    "in_dim = train_features.shape[1]\n",
    "print(in_dim)\n",
    "print(train_features.shape)\n",
    "print(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "worth-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = vectorizer.get_feature_names()\n",
    "dense = test_vecs.todense()\n",
    "denselist = dense.tolist()\n",
    "fe = pd.DataFrame(denselist, columns = names)\n",
    "\n",
    "X_test = X_test.drop(['Speech'], axis=1)\n",
    "test_features = np.hstack([X_test, fe])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-craps",
   "metadata": {},
   "source": [
    "5. The feed forward neural network is created with 2 hidden layers of 64/32 neurons and relu/sigmoid activation functions respectively. The output layer has the sigmoid activation function. The model is compiled with binary_crossentropy loss function and adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "thick-salmon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 10034)             100691190 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                642240    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101,335,543\n",
      "Trainable params: 101,335,543\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#keras NN model initialization\n",
    "\n",
    "model = keras.Sequential([\n",
    "    #input layer\n",
    "    keras.layers.Dense(in_dim,input_shape=(in_dim,)),\n",
    "    #hidden layer\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    #hidden layer\n",
    "    keras.layers.Dense(32, activation='sigmoid'),\n",
    "    #output layer\n",
    "    keras.layers.Dense(1, activation='sigmoid'),   \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['AUC'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ready-parish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000029BA672CF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000029BA672CF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1308/1308 [==============================] - 416s 317ms/step - loss: 0.6261 - auc: 0.6989\n",
      "82/82 [==============================] - 46s 561ms/step - loss: 0.5396 - auc: 0.8031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29bb375f6c8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features = np.asarray(train_features)\n",
    "y_train = np.asarray(y_train)    \n",
    "model.fit(train_features, y_train, epochs=1, batch_size=8)\n",
    "model.fit(train_features, y_train, epochs=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "limiting-memorabilia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000029BA71F0C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000029BA71F0C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Premise      0.650     0.619     0.634      3214\n",
      "       Claim      0.652     0.682     0.666      3361\n",
      "\n",
      "    accuracy                          0.651      6575\n",
      "   macro avg      0.651     0.650     0.650      6575\n",
      "weighted avg      0.651     0.651     0.651      6575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(test_features)\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i][0] > 0.5:\n",
    "        y_pred[i][0] = 1\n",
    "    else:\n",
    "        y_pred[i][0] = 0\n",
    "            \n",
    "target_names = ['Premise', 'Claim']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names, digits=3))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "hawaiian-disclaimer",
   "metadata": {},
   "source": [
    "                precision   recall  f1-score   support\n",
    "\n",
    "     Premise      0.650     0.619     0.634      3214\n",
    "       Claim      0.652     0.682     0.666      3361\n",
    "\n",
    "    accuracy                          0.651      6575\n",
    "   macro avg      0.651     0.650     0.650      6575\n",
    "weighted avg      0.651     0.651     0.651      6575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-sellers",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
