{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-shepherd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from nltk import tokenize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Bidirectional, Embedding, Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from gensim.models import FastText\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-cause",
   "metadata": {},
   "source": [
    "1. We import the raw dataset as a dataframe and process it to acquire for each entry, a tokenized sentence with with its corresponding label, 1 for argument, and 0 for non-argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-parent",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../../data/sentence_db_candidate.csv'\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-steam",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join([i for i in sentence if i not in string.punctuation])\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-canada",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Speech'] = df['Speech'].apply(preproc)\n",
    "valid = ['Claim', 'Premise']\n",
    "df = df.loc[(df['Component'].isin(valid))]\n",
    "\n",
    "#turning labels into two classes \n",
    "classes = []\n",
    "\n",
    "for s in df.Component:\n",
    "    if s == 'Claim':\n",
    "        classes.append(1.0)\n",
    "    else:\n",
    "        classes.append(0.0)\n",
    "        \n",
    "df['Annotation'] = classes\n",
    "df.Annotation.value_counts()\n",
    "df = df[['Speech', 'Annotation', 'Set']]\n",
    "\n",
    "df_train = df[df['Set'] == 'TRAIN']\n",
    "df_val = df[df['Set'] == 'VALIDATION']\n",
    "df_test = df[df['Set'] == 'TEST']\n",
    "\n",
    "all_sentences = df.iloc[:, 0].tolist()\n",
    "all_sentences_train = df_train.iloc[:, 0].tolist()\n",
    "all_sentences_test = df_test.iloc[:, 0].tolist()\n",
    "\n",
    "all_labels_train = df_train.iloc[:, 1].tolist()\n",
    "all_labels_test = df_test.iloc[:, 1].tolist()\n",
    "\n",
    "all_sent_train_tokenized = []\n",
    "all_sent_test_tokenized = []\n",
    "all_sent_tokenized = []\n",
    "longest_word_len = []\n",
    "for i in range (len(all_sentences)):\n",
    "    all_sent_tokenized.append(word_tokenize(all_sentences[i]))\n",
    "\n",
    "for i in range (len(all_sentences_train)):\n",
    "    all_sent_train_tokenized.append(word_tokenize(all_sentences_train[i]))\n",
    "    \n",
    "for i in range (len(all_sentences_test)):\n",
    "    all_sent_test_tokenized.append(word_tokenize(all_sentences_test[i]))\n",
    "    \n",
    "print(\"longest sentence: \", max(all_sent_tokenized,key=len))\n",
    "print(\"longest sentence length: \", len(max(all_sent_tokenized,key=len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-sugar",
   "metadata": {},
   "source": [
    "2. We import the fasttext embeddings and then represent each tokenized sentence as indices of the respective tokens in the embedding vocabulary. All sentences will be padded to match the length of the longest sentence in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "FT = \"../utils/wiki-news-300d-1M.vec\"\n",
    "fasttext = KeyedVectors.load_word2vec_format(FT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-surname",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 149\n",
    "vocab = Counter()\n",
    "\n",
    "for sent in all_sent_tokenized:\n",
    "    vocab.update(sent)\n",
    "    \n",
    "unique_words = len(fasttext)\n",
    "word_index = {t[0]: i+1 for i,t in enumerate(vocab.most_common(unique_words))}\n",
    "\n",
    "sequences_train = [[word_index.get(t, 0) for t in sent] for sent in all_sent_train_tokenized]\n",
    "sequences_test = [[word_index.get(t, 0) for t in sent] for sent in all_sent_test_tokenized]\n",
    "\n",
    "data_train = pad_sequences(sequences_train, maxlen=max_seq_len, padding=\"pre\", truncating=\"post\")\n",
    "print('Shape of data:', data_train.shape)\n",
    "\n",
    "data_test = pad_sequences(sequences_test, maxlen=max_seq_len, padding=\"pre\", truncating=\"post\")\n",
    "print('Shape of data:', data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-deposit",
   "metadata": {},
   "source": [
    "3. An embedding matrix will be created as the input embedding layer in the LSTM network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-holder",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = (np.random.rand(unique_words, 300) - 0.5) / 5.0\n",
    "for word, i in word_index.items():\n",
    "    if i >= unique_words:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = fasttext[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        pass  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-texture",
   "metadata": {},
   "source": [
    "4. The bidirectional LSTM network is created with with an embedding layer whose weights are the pretrained fasttext embeddings and has a dimension of 300. We use 200 neurons for the bidirectional LSTM layer, sigmoid activation function for the output layer, binary_crossentropy for the loss function, and adam optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('cpu:0'):\n",
    "  embedding_layer = Embedding(len(fasttext), 300, weights = [embedding_matrix] , trainable=False)\n",
    "  embedding_layer.build((len(fasttext), 300))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Bidirectional(LSTM(200, return_sequences=False, activation=\"sigmoid\"), input_shape=(300, 1)))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"AUC\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-italic",
   "metadata": {},
   "source": [
    "5. Train and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.asarray(data_train)\n",
    "all_labels_train = np.asarray(all_labels_train)\n",
    "model.fit(x=data_train, y=all_labels_train, epochs=1,batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(data_test)\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i][0] > 0.5:\n",
    "        y_pred[i][0] = 1\n",
    "    else:\n",
    "        y_pred[i][0] = 0\n",
    "        \n",
    "target_names = ['Premise', 'Claim']\n",
    "print(classification_report(all_labels_test, y_pred, target_names=target_names, digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
