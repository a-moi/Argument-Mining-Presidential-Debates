{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import spacy\n",
    "import collections\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import keras\n",
    "import tensorflow_addons as tfa\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-concentrate",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'sentence_db_candidate.csv'\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-sacramento",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join([i for i in sentence if i not in string.punctuation])\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Speech'] = df['Speech'].apply(preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = ['Claim', 'Premise', 'O']\n",
    "df = df.loc[(df['Component'].isin(valid))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-sport",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turning labels into two classes \n",
    "classes = []\n",
    "\n",
    "for s in df.Component:\n",
    "    if s == 'O':\n",
    "        classes.append(0.0)\n",
    "    else:\n",
    "        classes.append(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Annotation'] = classes\n",
    "df.Annotation.value_counts()\n",
    "df = df[['Speech', 'Annotation', 'Set']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-small",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_connectives (df, speech_sents):\n",
    "\n",
    "    \"\"\" \n",
    "    :input: df: entire DataFrame\n",
    "            speech_sents: numpy array of text data instances in DataFrame\n",
    "    :return: df: DataFrame with a new feature Claim_Connective, \n",
    "            representing the presence/absence of any connective from a given list in a sentence\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    connectives = ['so that', 'as a result', 'therefore', 'thus', 'thereby', 'in the end', 'hence', 'accordingly', 'in this way']\n",
    "    lst = []\n",
    "    \n",
    "    for sent in speech_sents:\n",
    "        if any(w in sent for w in connectives):\n",
    "            lst.append(1)\n",
    "        else:\n",
    "            lst.append(0)\n",
    "    df['Claim_Connective'] = lst\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-disney",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_connectives(df, df['Speech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment (df, speech_sents): \n",
    "    \n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    senti = []\n",
    "    \n",
    "    for sent in speech_sents:\n",
    "        vs = analyzer.polarity_scores(sent)\n",
    "        senti.append([list(vs.values())[3]])\n",
    "    \n",
    "    senti_arr = np.array(senti)\n",
    "    df['Sentiment'] = senti_arr\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-standard",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_sentiment(df, df['Speech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_loaded = spacy.load(\"en_core_web_sm\")\n",
    "# tag text and extract tags into a list\n",
    "\n",
    "df['ner'] = df['Speech'].apply(lambda x: [(tag.text, tag.label_) \n",
    "                                for tag in spacy_loaded(x).ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-double",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# utils function to count the element of a list\n",
    "\n",
    "def utils_lst_count(lst):\n",
    "    dic_counter = collections.Counter()\n",
    "    \n",
    "    for x in lst:\n",
    "        dic_counter[x] += 1\n",
    "    \n",
    "    dic_counter = collections.OrderedDict( \n",
    "                     sorted(dic_counter.items(), \n",
    "                     key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    lst_count = [{key:value} for key,value in dic_counter.items()]\n",
    "    \n",
    "    return lst_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count tags\n",
    "df['ner'] = df['ner'].apply(lambda x: utils_lst_count(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils function create new column for each tag category\n",
    "\n",
    "def utils_ner_features(lst_dics_tuples, tag):\n",
    "    if len(lst_dics_tuples) > 0:\n",
    "        tag_type = []\n",
    "        for dic_tuples in lst_dics_tuples:\n",
    "            for tuple in dic_tuples:\n",
    "                type, n = tuple[1], dic_tuples[tuple]\n",
    "                tag_type = tag_type + [type]*n\n",
    "                dic_counter = collections.Counter()\n",
    "                for x in tag_type:\n",
    "                    dic_counter[x] += 1\n",
    "        return dic_counter[tag]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-bullet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features\n",
    "\n",
    "tags_set = []\n",
    "\n",
    "for lst in df['ner'].tolist():\n",
    "    for dic in lst:\n",
    "        for k in dic.keys():\n",
    "            tags_set.append(k[1])\n",
    "            \n",
    "tags_set = list(set(tags_set))\n",
    "\n",
    "for feature in tags_set:\n",
    "    df['ner_' + feature] = df['ner'].apply(lambda x: utils_ner_features(x, feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-europe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['ner'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-debut",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pos'] = df['Speech'].apply(lambda x: [(tag.text, tag.pos_) \n",
    "                                for tag in spacy_loaded(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-brunswick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count tags\n",
    "df['pos'] = df['pos'].apply(lambda x: utils_lst_count(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-memorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract pos \n",
    "pos_set = []\n",
    "\n",
    "for lst in df['pos'].tolist():\n",
    "    for dic in lst:\n",
    "        for k in dic.keys():\n",
    "            pos_set.append(k[1])\n",
    "            \n",
    "pos_set = list(set(pos_set))\n",
    "\n",
    "for feature in pos_set:\n",
    "    df['pos_' + feature] = df['pos'].apply(lambda x: utils_ner_features(x, feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-superintendent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping only adverbs and adjectives and dropping other pos, like authors had\n",
    "for feature in df.columns:\n",
    "    if feature != 'pos_ADV' and feature != 'pos_ADJ' and 'pos' in feature:\n",
    "        df = df.drop(feature, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting as the authors did \n",
    "df_train = df[df['Set'] == 'TRAIN']\n",
    "df_val = df[df['Set'] == 'VALIDATION']\n",
    "df_test = df[df['Set'] == 'TEST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-confirmation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['Set'], axis=1)\n",
    "df_test = df_test.drop(['Set'], axis=1)\n",
    "\n",
    "X_train = df_train.drop(['Annotation'], axis=1)\n",
    "y_train = df_train.Annotation\n",
    "X_test = df_test.drop(['Annotation'], axis=1)\n",
    "y_test = df_test.Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = TfidfVectorizer(max_features=10000, ngram_range=(1,3))\n",
    "bow_train = bow.fit_transform(X_train['Speech'])\n",
    "bow_test = bow.transform(X_test['Speech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-chaos",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = bow.get_feature_names()\n",
    "dense = bow_train.todense()\n",
    "denselist = dense.tolist()\n",
    "fe = pd.DataFrame(denselist, columns = names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-madness",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(['Speech'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.hstack([X_train, fe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = train_features.shape[1]\n",
    "print(in_dim)\n",
    "print(train_features.shape)\n",
    "print(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-airport",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = bow.get_feature_names()\n",
    "dense = bow_test.todense()\n",
    "denselist = dense.tolist()\n",
    "fe = pd.DataFrame(denselist, columns = names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.drop(['Speech'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = np.hstack([X_test, fe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-salmon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras NN model initialization\n",
    "\n",
    "model = keras.Sequential([\n",
    "    #input layer\n",
    "    keras.layers.Dense(in_dim,input_shape=(in_dim,)),\n",
    "    #hidden layer\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    #hidden layer\n",
    "    keras.layers.Dense(32, activation='sigmoid'),\n",
    "    #output layer\n",
    "    keras.layers.Dense(1, activation='sigmoid'),   \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['Precision','Recall'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-parish",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.asarray(train_features)\n",
    "y_train = np.asarray(y_train)    \n",
    "model.fit(train_features, y_train, epochs=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-memorabilia",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_features)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-cookbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i][0] > 0.5:\n",
    "        y_pred[i][0] = 1\n",
    "    else:\n",
    "        y_pred[i][0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-theme",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['Not Argument', 'Is Argument']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-provision",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
