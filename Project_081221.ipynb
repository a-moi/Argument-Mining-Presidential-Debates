{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "affecting-superior",
   "metadata": {},
   "source": [
    "<h2>Load the data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "missing-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from afinn import Afinn\n",
    "from sklearn import svm\n",
    "from nltk import tokenize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "auburn-mixer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#edit line 3 and line 8 to narrow the selection of files to be read in\n",
    "txt_files = []\n",
    "for name in glob.glob('ElecDeb60To16/*.txt'):\n",
    "    name = name.replace('\\\\','/')\n",
    "    txt_files.append(name)\n",
    "\n",
    "ann_files = []\n",
    "for name in glob.glob('ElecDeb60To16/*.ann'):\n",
    "    name = name.replace('\\\\','/')\n",
    "    ann_files.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "derived-jewel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This block reads in all the sentences and annotations \n",
    "sent_all = []\n",
    "ind_all = []\n",
    "\n",
    "for file in txt_files:\n",
    "    sent_list = []\n",
    "    ind_list = []\n",
    "    prev_ind = 0\n",
    "    \n",
    "    txt_file = open(file)\n",
    "    text = txt_file.read()\n",
    "    txt_file.close()\n",
    "    \n",
    "    \n",
    "    with open(file) as infile:\n",
    "        for line in infile:\n",
    "            line = re.sub(r'(:.+ANCHORRRR)','',line)\n",
    "            line = re.sub(r'(\\([A-Z]+\\)_[A-Z]+(\\_[A-Z]+)?: )','',line)\n",
    "            line = re.sub(r'(\\([A-Z\\s]+:[A-Za-z\\.\\s]+\\))','',line)\n",
    "            line = re.sub(r'([A-Z]+\\_[A-Z]+\\_[A-Z]+: )','',line)\n",
    "            line = re.sub(r'([A-Z]+\\_[A-Z]+: )','',line)\n",
    "            line = re.sub(r'([A-Z]+\\.\\s[A-Z]+: )','',line)\n",
    "            line = re.sub(r'([A-Z]+\\'[A-Z]+: )','',line)\n",
    "            line = re.sub(r'(\\_[A-Z]+: )','',line)\n",
    "            line = re.sub(r'([A-Z]+: )','',line)\n",
    "            line = re.sub(r'\\[[a-zA-Z0-9;,\\s\\\"\\'\\.\\S]*\\]*','',line)\n",
    "            \n",
    "                  \n",
    "            for sent in tokenize.sent_tokenize(line):\n",
    "                if not sent_list:\n",
    "                    ind = text.find(sent,0,len(text)) \n",
    "                else:\n",
    "                    ind = text.find(sent,prev_ind,len(text))\n",
    "                prev_ind = ind\n",
    "                sent_list.append([sent,ind])\n",
    "                ind_list.append(ind)\n",
    "                \n",
    "    ind_all.append(np.array(ind_list))\n",
    "    sent_all.append(sent_list)\n",
    "    \n",
    "ann_all = []\n",
    "\n",
    "for file in ann_files:\n",
    "    ann_list = []\n",
    "    \n",
    "    with open(file) as infile:\n",
    "        for line in infile:\n",
    "            entry = line.split(\"\\t\")\n",
    "            ann_list.append(entry)\n",
    "    \n",
    "    ann_all.append(ann_list)\n",
    "\n",
    "#Up to this point we get a sent_all containing every debate (42 total) tokenized into sentences.\n",
    "#sent_all = [sent_list, sent_list, sent_list,...]\n",
    "#sent_list = [[sent1, index],[sent2, index],[sent3,index],...]\n",
    "#ind_all = [ind_list, ind_list, ind_list,...]\n",
    "#ind_list = [ind, ind, ind,...]\n",
    "\n",
    "#ann_all = [ann_list, ann_list, ann_list, ...]\n",
    "#ann_list = [['T1', 'Claim/Premise xxx xxx', '...'], ['T2', 'Claim/Premise xxx xxx', '...'], \n",
    "    #['T3', 'Claim/Premise xxx xxx', '...'], ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unexpected-growing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#following part creates labels for task 1 (whether a sentence contains an argument component)\n",
    "\n",
    "labels_arg_all = []\n",
    "\n",
    "for i in range(len(ind_all)):\n",
    "    labels_arg_arr = np.empty([len(ind_all[i]),2])\n",
    "    sent_start_ind_list = []\n",
    "\n",
    "    #column 0 of labels_arg are the starting indices of the senteces \n",
    "    for j in range(len(ind_all[i])):\n",
    "        labels_arg_arr[j,0] = ind_all[i][j]\n",
    "        \n",
    "    #loop through each annotation entry, get the corresponding sentence starting index \n",
    "    for j in range(len(ann_all[i])):\n",
    "        phrase_start = re.search(r'[0-9]+\\s',ann_all[i][j][1]).group(0)\n",
    "        sent_start_ind_list.append(ind_all[i][ind_all[i] <= int(phrase_start)].max())\n",
    "        \n",
    "        phrase_start = re.search(r';[0-9]+\\s',ann_all[i][j][1])\n",
    "        if phrase_start != None:\n",
    "            phrase_start = phrase_start.group(0)[1:]\n",
    "            ind2 = ind_all[i][ind_all[i] <= int(phrase_start)].max()\n",
    "            if ind2 not in sent_start_ind_list:\n",
    "                sent_start_ind_list.append(ind2)\n",
    "    \n",
    "    #loop through starting indices for sentences, check if each sentence contains an annotation (labeled as 1)\n",
    "    for j in range(len(ind_all[i])):   \n",
    "        if ind_all[i][j] in sent_start_ind_list:\n",
    "            labels_arg_arr[j,1] = 1\n",
    "        else:\n",
    "            labels_arg_arr[j,1] = 0\n",
    "    \n",
    "    labels_arg_all.append(labels_arg_arr.astype(int))\n",
    "    \n",
    "#following part creates labels for task 2 (whether a sentence contains an premise or a claim or none)\n",
    "    \n",
    "labels_cp_all = []\n",
    "\n",
    "for i in range(len(labels_arg_all)):\n",
    "    labels_cp_list = []\n",
    "    \n",
    "    for j in range(labels_arg_all[i].shape[0]):\n",
    "        if labels_arg_all[i][j,1] != 0:\n",
    "            labels_cp_list.append(labels_arg_all[i][j,0])\n",
    "    \n",
    "    labels_cp_arr = np.zeros((len(labels_cp_list),3))\n",
    "    labels_cp_out = np.zeros((len(labels_cp_list),2))\n",
    "    \n",
    "    for k in range(len(labels_cp_list)):\n",
    "        labels_cp_arr[k,0] = labels_cp_list[k]\n",
    "\n",
    "        \n",
    "    for j in range(len(ann_all[i])):\n",
    "        #check for claim with only 1 continuous part\n",
    "        phrase_start = re.search(r'c|Claim\\s[0-9]+\\s[0-9]+',ann_all[i][j][1])\n",
    "        if phrase_start != None:\n",
    "            phrase_start = phrase_start.group(0)\n",
    "            labels_cp_arr[np.where(labels_cp_arr[:,0] == ind_all[i][ind_all[i] <= int(phrase_start.split()[1])].max())[0],1] +=\\\n",
    "            int(phrase_start.split()[2]) - int(phrase_start.split()[1])\n",
    "        \n",
    "        #check for claim formed from 2 separate parts\n",
    "        phrase_start = re.search(r'c|Claim[0-9\\s]+;[0-9]+\\s[0-9]+',ann_all[i][j][1])\n",
    "        if phrase_start != None:\n",
    "            phrase_start = phrase_start.group(0)\n",
    "            labels_cp_arr[np.where(labels_cp_arr[:,0] == ind_all[i][ind_all[i] <= int(phrase_start.split(';')[1].split()[0])].max())[0],1] +=\\\n",
    "            int(phrase_start.split(';')[1].split()[1]) - int(phrase_start.split(';')[1].split()[0])\n",
    "        \n",
    "        #check for premise with only 1 continuous part\n",
    "        phrase_start = re.search(r'p|Premise\\s[0-9]+\\s[0-9]+',ann_all[i][j][1])\n",
    "        if phrase_start != None:\n",
    "            phrase_start = phrase_start.group(0)\n",
    "            labels_cp_arr[np.where(labels_cp_arr[:,0] == ind_all[i][ind_all[i] <= int(phrase_start.split()[1])].max())[0],2] +=\\\n",
    "            int(phrase_start.split()[2]) - int(phrase_start.split()[1])\n",
    "        \n",
    "        #check for premise formed from 2 separate parts\n",
    "        phrase_start = re.search(r'p|Premise[0-9\\s]+;[0-9]+\\s[0-9]+',ann_all[i][j][1])\n",
    "        if phrase_start != None:\n",
    "            phrase_start = phrase_start.group(0)\n",
    "            labels_cp_arr[np.where(labels_cp_arr[:,0] == ind_all[i][ind_all[i] <= int(phrase_start.split(';')[1].split()[0])].max())[0],2] +=\\\n",
    "            int(phrase_start.split(';')[1].split()[1]) - int(phrase_start.split(';')[1].split()[0])\n",
    "    \n",
    "    #compare length of claim and premise components in a sentence, and label the sentence according to the longer component\n",
    "    labels_cp_out[:,0] = labels_cp_arr[:,0]\n",
    "    for m in range(labels_cp_arr.shape[0]):\n",
    "        if labels_cp_arr[m,1] >= labels_cp_arr[m,2]:\n",
    "            labels_cp_out[m,1] = 1\n",
    "            \n",
    "    labels_cp_all.append(labels_cp_out)\n",
    "\n",
    "#labels_arg_all = [labels_arg_arr, labels_arg_arr, labesl_arg_arr, ...]\n",
    "#labels_arg_arr = np array of size (len(ind_list),2), no. of rows = no. of sentences in a txt file, \n",
    "    #col 0 = starting index of sentece, col 1 = label (1 for contain claim/premise, 0 for otherwise)  \n",
    "      \n",
    "#labels_cp_all = [labels_cp_out, labels_cp_out, labels_cp_out, ...]\n",
    "#labels_cp_out = np array of size ((len(labels_cp_list),3)), \n",
    "    #no. of rows = no. sentences that contains at least 1 claim or 1 premise,\n",
    "    #col 0 = starting index of such a sentence, col 1 = label (1 for claim, 0 for premise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pressed-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'updated_csv.csv'\n",
    "df = pd.read_csv(filename)\n",
    "df2 = df[df.Annotation != 'None']\n",
    "\n",
    "#task 1, compile all sentences(for feature engineering) and corresponding labels, 1 for containing argument component\n",
    "all_sentences = df.iloc[:, 1].tolist()\n",
    "all_labels = df.iloc[:, 2].tolist()\n",
    "for i in range(len(all_labels)):\n",
    "    if all_labels[i] == \"Claim\" or all_labels[i] == \"Premise\":\n",
    "        all_labels[i] = 1\n",
    "    else:\n",
    "        all_labels[i] = 0\n",
    "        \n",
    "#task 2, compile only sentences containing claim/premise(for feature engineering) and corresponding labels, 1 for claim\n",
    "cp_sentences = df2.iloc[:, 1].tolist()\n",
    "cp_labels = df2.iloc[:, 2].tolist()\n",
    "for i in range(len(cp_labels)):\n",
    "    if cp_labels[i] == \"Claim\":\n",
    "        cp_labels[i] = 1\n",
    "    else:\n",
    "        cp_labels[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "marked-israeli",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "all_features = []\n",
    "for sentence in all_sentences:\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    all_features.append(list(vs.values()))\n",
    "    \n",
    "#print(all_features)\n",
    "    \n",
    "#################################################################\n",
    "cp_features = []\n",
    "for sentence in cp_sentences:\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    cp_features.append(list(vs.values()))\n",
    "\n",
    "#print(cp_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "basic-sweden",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn, fp, fn, tp:  [   0 3251    0 5800]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_true = train_test_split(all_features, all_labels, test_size=0.25, random_state=42)\n",
    "\n",
    "#tinkering with svm\n",
    "clf = svm.SVC(kernel='linear', random_state = 1).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"tn, fp, fn, tp: \",confusion_matrix(y_true, y_pred).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "excellent-language",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras NN model initialization\n",
    "\n",
    "model = keras.Sequential([\n",
    "    #input layer\n",
    "    keras.layers.Dense(4,input_shape=(4,)),\n",
    "    #hidden layer\n",
    "    keras.layers.Dense(20, activation='relu'),\n",
    "    #output layer\n",
    "    keras.layers.Dense(2, activation='softmax'),   \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-monday",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels_cat = to_categorical(all_labels)\n",
    "all_features_arr = np.array(all_features)\n",
    "cp_labels_cat = to_categorical(cp_labels)\n",
    "cp_features_arr = np.array(cp_features)\n",
    "\n",
    "#tinkering with keras NN\n",
    "X_train, X_test, y_train, y_true = train_test_split(all_features_arr, all_labels_cat, test_size=0.25, random_state=42)\n",
    "model_fit = model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-williams",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
