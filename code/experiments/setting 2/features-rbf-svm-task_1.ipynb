{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Setting 2: Features + rbf SVM \n",
    "### Task 1: Classification Argument (contains either Claim or Premise) vs non-Argument "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing necessary packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mariap/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mariap/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import collections\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag, word_tokenize, RegexpParser\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../../../data/sentence_db_candidate.csv'\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple preprocessing of sentences with lowercasing and punctuation removal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join([i for i in sentence if i not in string.punctuation])\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Speech'] = df['Speech'].apply(preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = ['Claim', 'Premise', 'O']\n",
    "df = df.loc[(df['Component'].isin(valid))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below we are turning labels marking Claims, Premises and None into machine-readable classes: 1 for claims and premises and 0 for none**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "\n",
    "for s in df.Component:\n",
    "    if s == 'O':\n",
    "        classes.append(0.0)\n",
    "    else:\n",
    "        classes.append(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    22280\n",
       "0.0     7252\n",
       "Name: Annotation, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Annotation'] = classes\n",
    "df['Annotation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Speech', 'Annotation', 'Set']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speech</th>\n",
       "      <th>Annotation</th>\n",
       "      <th>Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gwen i want to thank you and i want to thank ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>its a very important event and theyve done a s...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>its important to look at all of our developmen...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and after 911 it became clear that we had to d...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and we also then finally had to stand up democ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Speech  Annotation    Set\n",
       "0   gwen i want to thank you and i want to thank ...         0.0  TRAIN\n",
       "1  its a very important event and theyve done a s...         0.0  TRAIN\n",
       "2  its important to look at all of our developmen...         0.0  TRAIN\n",
       "3  and after 911 it became clear that we had to d...         1.0  TRAIN\n",
       "4  and we also then finally had to stand up democ...         1.0  TRAIN"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we are starting feature engineering. The functions are designed to take the full dataframe and a column with sentences and to output a dataframe with a newly added feature(s). Please refer to `linguistic_features.py` file to see/copy the documented feature functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Features: Part-of-Speech (adverbs and adjectives)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_count(lst):\n",
    "    \n",
    "    \"\"\"\n",
    "    :function: count the elements of a list -- the number of words with a respective POS or NER labels in a sentence. \n",
    "    :input: lst: list of tuples, where tuple has two elements -- a word and its POS or NER label\n",
    "    :return: lst_count: list of dictionaries, where\n",
    "    the dictionary consists of keys -- the elements are words and their POS or NER labels\n",
    "    and values -- how many times each word and its POS or NER label occurs\n",
    "    If a sentence has no POS or NER labels, return an empty list \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    dic_counter = collections.Counter()\n",
    "    \n",
    "    for x in lst:\n",
    "        dic_counter[x] += 1\n",
    "    \n",
    "    dic_counter = collections.OrderedDict( \n",
    "                     sorted(dic_counter.items(), \n",
    "                     key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    lst_count = [{key:value} for key,value in dic_counter.items()]\n",
    "    \n",
    "    return lst_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_tag(lst_dics_tuples, tag):\n",
    "    \n",
    "    \"\"\"\n",
    "    :function: new column for each POS or NER tag category \n",
    "    :input: lst_dics_tuples: list of dictionaries with tuples \n",
    "            tag: POS or NER label from a list\n",
    "    :return: tag: new column for each POS or NER label with their counts\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(lst_dics_tuples) > 0:\n",
    "        tag_type = []\n",
    "        \n",
    "        for dic_tuples in lst_dics_tuples:\n",
    "            for tuple in dic_tuples:\n",
    "                type, n = tuple[1], dic_tuples[tuple]\n",
    "                tag_type = tag_type + [type]*n\n",
    "                dic_counter = collections.Counter()\n",
    "                for x in tag_type:\n",
    "                    dic_counter[x] += 1\n",
    "        return dic_counter[tag]\n",
    "    \n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features (df, speech_sents):\n",
    "    \n",
    "    \"\"\"\n",
    "    :function: add two new columns with two POS: adjectives and adverbs, and their counts per sentence.\n",
    "    Two helper functions -- list_count, column_tag -- are needed \n",
    "    :input: df: entire DataFrame\n",
    "            speech_sents: Series of sentences in DataFrame\n",
    "    :return: df: new DataFrame with two new features\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    df['pos'] = speech_sents.apply(lambda x: [(tag.text, tag.pos_) \n",
    "                                for tag in nlp(x)])\n",
    "    \n",
    "    df['pos'] = df['pos'].apply(lambda x: list_count(x))\n",
    "    \n",
    "    #extract features\n",
    "    tags_set = []\n",
    "\n",
    "    for lst in df['pos'].tolist():\n",
    "        for dic in lst:\n",
    "            for k in dic.keys():\n",
    "                tags_set.append(k[1])\n",
    "            \n",
    "    tags_set = list(set(tags_set))\n",
    "\n",
    "    for feature in tags_set:\n",
    "        df['pos_' + feature] = df['pos'].apply(lambda x: column_tag(x, feature))\n",
    "        \n",
    "    # keeping only adverbs and adjectives and dropping other pos\n",
    "    for feature in df.columns:\n",
    "        if feature != 'pos_ADV' and feature != 'pos_ADJ' and 'pos' in feature:\n",
    "            df = df.drop(feature, axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pos_features(df, df['Speech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Features: Named Entity Recognition labels*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_features(df, speech_sents):\n",
    "    \n",
    "    \"\"\"\n",
    "    :function: add several new columns with NER labels, and their counts per sentence.\n",
    "    Two helper functions -- list_count, column_tag -- are needed \n",
    "    :input: df: entire DataFrame\n",
    "            speech_sents: Series of sentences in DataFrame\n",
    "    :return: df: new DataFrame with new features for each NER label\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    df['ner'] = speech_sents.apply(lambda x: [(tag.text, tag.label_) \n",
    "                                for tag in nlp(x).ents])\n",
    "    # count tags\n",
    "    df['ner'] = df['ner'].apply(lambda x: list_count(x))\n",
    "    \n",
    "    # extract features\n",
    "    tags_set = []\n",
    "\n",
    "    for lst in df['ner'].tolist():\n",
    "        for dic in lst:\n",
    "            for k in dic.keys():\n",
    "                tags_set.append(k[1])\n",
    "            \n",
    "    tags_set = list(set(tags_set))\n",
    "\n",
    "    for feature in tags_set:\n",
    "        df['ner_' + feature] = df['ner'].apply(lambda x: column_tag(x, feature))\n",
    "        \n",
    "    df = df.drop(['ner'], axis=1)\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ner_features(df, df['Speech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Features: Verbs' tenses and modals*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbs_features (df, speech_sents):\n",
    "    \n",
    "    \"\"\"\n",
    "    :function: add several new columns with features for verb tenses and the presence of modal verbs, \n",
    "    and their counts per sentence.\n",
    "    Two helper functions -- list_count, column_tag -- are needed \n",
    "    :input: df: entire DataFrame\n",
    "            speech_sents: Series of sentences in DataFrame\n",
    "    :return: df: new DataFrame with features for each verb tense and for modal verbs\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    df['verb_tag'] = speech_sents.apply(lambda x: [(tag.text, tag.tag_) \n",
    "                                for tag in nlp(x)])\n",
    "    \n",
    "    df['verb_tag'] = df['verb_tag'].apply(lambda x: list_count(x))\n",
    "    \n",
    "    #extract features\n",
    "    verbs_set = []\n",
    "\n",
    "    for lst in df['verb_tag'].tolist():\n",
    "        for dic in lst:\n",
    "            for k in dic.keys():\n",
    "                verbs_set.append(k[1])\n",
    "            \n",
    "    verbs_set = list(set(verbs_set))\n",
    "\n",
    "    for feature in verbs_set:\n",
    "        df['verb_tag_' + feature] = df['verb_tag'].apply(lambda x: column_tag(x, feature))\n",
    "    \n",
    "    #out of all detailed POS tags, keeping only verbs-related ones \n",
    "    for f in df.columns:\n",
    "        if f != 'verb_tag_VB' and f != 'verb_tag_VBZ' and f != 'verb_tag_VBP' and f != 'verb_tag_VBD' and f != 'verb_tag_VBN' and f != 'verb_tag_VBG' and f != 'verb_tag_MD' and 'verb_tag' in f:\n",
    "            df = df.drop(f, axis=1)\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = verbs_features(df, df['Speech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Features: Syntactic features (the number of productions, the number of Verbal Phrases groups, the depth of a sentence tree)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syntactic_features(df, speech_sents):\n",
    "        \n",
    "    \"\"\"\n",
    "    :function: add syntactic features -- 1) the number of productions, 2) the number of VP groups per sentence, \n",
    "    and 3) the depth of a sentence tree \n",
    "    :input: df: entire DataFrame\n",
    "            speech_sents: Series of sentences in DataFrame\n",
    "    :return: df: new DataFrame with three syntactic features \n",
    "\n",
    "    \"\"\"\n",
    "  \n",
    "    a, b , c, d, e = [], [], [], [], []\n",
    "    for x, y in enumerate(speech_sents):\n",
    "        tagged = pos_tag(word_tokenize(y))\n",
    "        chunker = RegexpParser(r\"\"\"\n",
    "            NBAR:\n",
    "            {<NN.*|JJ>*<NN.*>}  \n",
    "            VP:\n",
    "            {<V.*>}  \n",
    "            NP:\n",
    "            {<NBAR>}\n",
    "            {<NBAR><IN><NBAR>}  \n",
    "        \"\"\")\n",
    "    \n",
    "        a.append(chunker.parse(tagged))\n",
    "        b.append(len(chunker.parse(tagged).productions()))\n",
    "        e.append(chunker.parse(tagged).productions())\n",
    "        c.append(chunker.parse(tagged).height())\n",
    "\n",
    "    df.loc[:, 'Speech_parsed'] = a\n",
    "    df.loc[:, 'Productions_count'] = b\n",
    "    df.loc[:, 'Tree_depth'] = c\n",
    "  \n",
    "\n",
    "    for i in e:\n",
    "        vp = []\n",
    "        for u in i:\n",
    "            if str(u).startswith('VP'):\n",
    "                vp.append(u)\n",
    "        d.append(len(vp))\n",
    "  \n",
    "    df.loc[:, 'VP_count'] = d\n",
    "    \n",
    "    df = df.drop(['Speech_parsed'], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariap/anaconda3/lib/python3.7/site-packages/pandas/core/dtypes/missing.py:517: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  val = np.array(val, copy=False)\n",
      "/Users/mariap/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3162: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return asarray(a).ndim\n"
     ]
    }
   ],
   "source": [
    "df = syntactic_features(df, df['Speech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Feature: Sentiment of a sentence*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment (df, speech_sents): \n",
    "    \n",
    "    \n",
    "    \"\"\" \n",
    "    :function: add a feature with a sentiment score for each sentence \n",
    "    :input: df: entire DataFrame\n",
    "            speech_sents: Series of sentences in DataFrame\n",
    "    :return: df: DataFrame with a new feature Sentiment\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    senti = []\n",
    "    \n",
    "    for sent in speech_sents:\n",
    "        vs = analyzer.polarity_scores(sent)\n",
    "        senti.append([list(vs.values())[3]])\n",
    "    \n",
    "    senti_arr = np.array(senti)\n",
    "    df['Sentiment'] = senti_arr\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_sentiment(df, df['Speech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Feature: Discourse connectives*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_connectives (df, speech_sents):\n",
    "\n",
    "    \"\"\" \n",
    "    :function: add a boolean feature based on presence/absence of a claim connective from the pre-defined list \n",
    "    :input: df: entire DataFrame\n",
    "            speech_sents: Series of sentences in DataFrame\n",
    "    :return: df: DataFrame with a new feature Claim_Connective\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    connectives = ['so that', 'as a result', 'therefore', 'thus', 'thereby', 'in the end', 'hence', 'accordingly', 'in this way', 'because', 'now that', 'insofar as', 'given that', 'in response to', 'consequently', 'as a consequence']\n",
    "    lst = []\n",
    "    \n",
    "    for sent in speech_sents:\n",
    "        if any(w in sent for w in connectives):\n",
    "            lst.append(1)\n",
    "        else:\n",
    "            lst.append(0)\n",
    "    df['Claim_Connective'] = lst\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_connectives(df, df['Speech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Features: Personal pronouns*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_personal(df, speech_sents):\n",
    "    \"\"\"\n",
    "    :function: add two boolean features based on the presence/absence of any pronoun from two given lists.\n",
    "    :input: df: entire DataFrame\n",
    "            speech_sents: Series of sentences in DataFrame\n",
    "    :return: df: DataFrame with two new features Pronoun_Singular and Pronoun_Plural\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    singular = [' i ', ' me ', ' my ', ' myself ', ' mine ']\n",
    "    plural = [' we ', ' our ', ' ours ', ' ourselves ']\n",
    "    lst_sing = []\n",
    "    lst_plur = []\n",
    "\n",
    "    for sent in speech_sents:\n",
    "        if any(w in sent for w in singular):\n",
    "            lst_sing.append(1)\n",
    "        else:\n",
    "            lst_sing.append(0)\n",
    "    df['Pronoun_Singular'] = lst_sing\n",
    "\n",
    "    for sent in speech_sents:\n",
    "        if any(w in sent for w in plural):\n",
    "            lst_plur.append(1)\n",
    "        else:\n",
    "            lst_plur.append(0)\n",
    "    df['Pronoun_Plural'] = lst_plur\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_personal(df, df['Speech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf uni-, bi- and tri-grams and training preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting the data into three sets. Our sets will be identical to those of authors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['Set'] == 'TRAIN']\n",
    "df_val = df[df['Set'] == 'VALIDATION']\n",
    "df_test = df[df['Set'] == 'TEST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14044, 37)\n",
      "(7033, 37)\n",
      "(8455, 37)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(14044, 37)\n",
    "(7033, 37)\n",
    "(8455, 37)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['Set'], axis=1)\n",
    "df_val = df_val.drop(['Set'], axis=1)\n",
    "df_test = df_test.drop(['Set'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separating features set and target variable set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(['Annotation'], axis=1)\n",
    "y_train = df_train.Annotation\n",
    "\n",
    "X_val = df_val.drop(['Annotation'], axis=1)\n",
    "y_val = df_val.Annotation\n",
    "\n",
    "X_test = df_test.drop(['Annotation'], axis=1)\n",
    "y_test = df_test.Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initializing tf-idf feature matrix. Fitting and transforming sentences on a train set and only transforming on a validation and test sets. Here we are using unigrams as well as bigrams and trigrams, set by the parameter `ngram_range`. Following the authors' practice, we will be using only the top occuring unigrams from the vocabulary, 10.000 n-grams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,3))\n",
    "\n",
    "#tf-idf\n",
    "train_vecs =  vectorizer.fit_transform(X_train['Speech'])\n",
    "val_tfidf = vectorizer.transform(X_val['Speech'])\n",
    "test_vecs = vectorizer.transform(X_test['Speech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformations of tf-idf matrix for further concatenation with other features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = vectorizer.get_feature_names()\n",
    "dense = train_vecs.todense()\n",
    "denselist = dense.tolist()\n",
    "fe = pd.DataFrame(denselist, columns = names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(['Speech'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.hstack([X_train, fe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing preparations\n",
    "names = vectorizer.get_feature_names()\n",
    "dense = test_vecs.todense()\n",
    "denselist = dense.tolist()\n",
    "fe = pd.DataFrame(denselist, columns = names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.drop(['Speech'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = np.hstack([X_test, fe])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Replication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First of all, repeating the authors' setting. Kernel is `rbf`, penalty parameter `C=10`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, random_state=42)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC(kernel='rbf', C=10, random_state=42)\n",
    "svm.fit(train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_svm = svm.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**class 1 stands for ARGUMENT, class 0 stands for NONE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0      0.807     0.265     0.399      1880\n",
      "     class 1      0.824     0.982     0.896      6575\n",
      "\n",
      "    accuracy                          0.822      8455\n",
      "   macro avg      0.815     0.623     0.647      8455\n",
      "weighted avg      0.820     0.822     0.785      8455\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_test, y_pred_test_svm, target_names=target_names, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "               precision    recall  f1-score   support\n",
    "\n",
    "     class 0      0.807     0.265     0.399      1880\n",
    "     class 1      0.824     0.982     0.896      6575\n",
    "\n",
    "    accuracy                          0.822      8455\n",
    "    macro avg     0.815     0.623     0.647      8455\n",
    "    weighted avg  0.820     0.822     0.785      8455\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our results are close to authors', but not as high. We perform hyperparameter tuning to improve the scores.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation set preparation\n",
    "names = vectorizer.get_feature_names()\n",
    "dense = val_tfidf.todense()\n",
    "denselist = dense.tolist()\n",
    "fe = pd.DataFrame(denselist, columns = names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_val.drop(['Speech'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features = np.hstack([X_val, fe])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First, initializing the parameters grid. We are tuning parameters `C` and `gamma`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [1, 10], 'gamma': [1, 0.1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] C=1, gamma=1 ....................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................................... C=1, gamma=1, total= 9.0min\n",
      "[CV] C=1, gamma=1 ....................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  9.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................................... C=1, gamma=1, total= 9.1min\n",
      "[CV] C=1, gamma=1 ....................................................\n",
      "[CV] ..................................... C=1, gamma=1, total= 8.3min\n",
      "[CV] C=1, gamma=1 ....................................................\n",
      "[CV] ..................................... C=1, gamma=1, total= 8.2min\n",
      "[CV] C=1, gamma=1 ....................................................\n",
      "[CV] ..................................... C=1, gamma=1, total= 8.2min\n",
      "[CV] C=1, gamma=0.1 ..................................................\n",
      "[CV] ................................... C=1, gamma=0.1, total= 6.8min\n",
      "[CV] C=1, gamma=0.1 ..................................................\n",
      "[CV] ................................... C=1, gamma=0.1, total= 7.8min\n",
      "[CV] C=1, gamma=0.1 ..................................................\n",
      "[CV] ................................... C=1, gamma=0.1, total= 8.0min\n",
      "[CV] C=1, gamma=0.1 ..................................................\n",
      "[CV] ................................... C=1, gamma=0.1, total= 7.5min\n",
      "[CV] C=1, gamma=0.1 ..................................................\n",
      "[CV] ................................... C=1, gamma=0.1, total= 7.4min\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV] .................................... C=10, gamma=1, total= 8.8min\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV] .................................... C=10, gamma=1, total= 8.8min\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV] .................................... C=10, gamma=1, total= 9.0min\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV] .................................... C=10, gamma=1, total=10.1min\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV] .................................... C=10, gamma=1, total= 8.8min\n",
      "[CV] C=10, gamma=0.1 .................................................\n",
      "[CV] .................................. C=10, gamma=0.1, total= 8.0min\n",
      "[CV] C=10, gamma=0.1 .................................................\n",
      "[CV] .................................. C=10, gamma=0.1, total= 9.2min\n",
      "[CV] C=10, gamma=0.1 .................................................\n",
      "[CV] .................................. C=10, gamma=0.1, total= 8.2min\n",
      "[CV] C=10, gamma=0.1 .................................................\n",
      "[CV] .................................. C=10, gamma=0.1, total= 8.2min\n",
      "[CV] C=10, gamma=0.1 .................................................\n",
      "[CV] .................................. C=10, gamma=0.1, total= 8.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed: 167.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(), param_grid={'C': [1, 10], 'gamma': [1, 0.1]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = GridSearchCV(SVC(kernel='rbf'), param_grid, refit=True, verbose=2)\n",
    "grid.fit(val_features, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'gamma': 0.1}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`'C': 1, 'gamma': 0.1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Training and testing the model with the best parameters; a linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As the tuning above shows, the best parameters are `C=1` and `gamma=0.1`. Now we shall train and test the model with this parameter setting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_best = SVC(kernel='rbf', C=1, gamma=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, gamma=0.1, random_state=42)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_best.fit(train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_svm_best = svm_best.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0      0.757     0.263     0.391      1880\n",
      "     class 1      0.822     0.976     0.893      6575\n",
      "\n",
      "    accuracy                          0.817      8455\n",
      "   macro avg      0.790     0.620     0.642      8455\n",
      "weighted avg      0.808     0.817     0.781      8455\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_test, y_pred_test_svm_best, target_names=target_names, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                precision    recall  f1-score   support\n",
    "\n",
    "     class 0      0.757     0.263     0.391      1880\n",
    "     class 1      0.822     0.976     0.893      6575\n",
    "\n",
    "    accuracy                          0.817      8455\n",
    "    macro avg     0.790     0.620     0.642      8455\n",
    "    weighted avg  0.808     0.817     0.781      8455"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unfortunately, the model with best parameters shown on the validation set, did not result in performance improvement on the testing set (compared to the original model above). To still try to improve the scores, we shall try `linear` kernel with default `C` and `gamma`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_lin = SVC(kernel='linear', C=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, kernel='linear', random_state=42)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_lin.fit(train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_svm_lin = svm_lin.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0      0.700     0.387     0.498      1880\n",
      "     class 1      0.845     0.953     0.895      6575\n",
      "\n",
      "    accuracy                          0.827      8455\n",
      "   macro avg      0.772     0.670     0.697      8455\n",
      "weighted avg      0.812     0.827     0.807      8455\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_test, y_pred_test_svm_lin, target_names=target_names, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "               precision    recall  f1-score   support\n",
    "\n",
    "     class 0      0.700     0.387     0.498      1880\n",
    "     class 1      0.845     0.953     0.895      6575\n",
    "\n",
    "    accuracy                          0.827      8455\n",
    "    macro avg     0.772     0.670     0.697      8455\n",
    "    weighted avg  0.812     0.827     0.807      8455\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion: With linear kernel, we got closer to the original results. Our average f1-score for both classes is 0.807 while the one of authors is 0.823. For non-argumentative class, our f1-score is higher: 0.498 vs 0.433**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
