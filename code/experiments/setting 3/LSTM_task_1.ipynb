{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "finnish-alabama",
   "metadata": {},
   "source": [
    "## Experimental Setting 3: fasttext embedding + LSTM Network \n",
    "### Task 1: Classification Argument (contains either Claim or Premise) vs non-Argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "portuguese-shepherd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from nltk import tokenize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Bidirectional, Embedding, Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from gensim.models import FastText\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-wayne",
   "metadata": {},
   "source": [
    "**We import the raw dataset as a dataframe and process it to acquire for each entry, a tokenized sentence with with its corresponding label, 1 for argument, and 0 for non-argument**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "included-parent",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../../../data/sentence_db_candidate.csv'\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cooperative-steam",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join([i for i in sentence if i not in string.punctuation])\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "primary-canada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longest sentence:  ['now', 'when', 'we', 'have', 'a', 'presidential', 'candidate', 'for', 'example', 'senator', 'kennedy', 'stating', 'over', 'and', 'over', 'again', 'that', 'the', 'united', 'states', 'is', 'second', 'in', 'space', 'and', 'the', 'fact', 'of', 'the', 'matter', 'is', 'that', 'the', 'space', 'score', 'today', 'is', 'twentyeight', 'to', 'eight', 'weve', 'had', 'twentyeight', 'successful', 'shots', 'theyve', 'had', 'eight', 'when', 'he', 'states', 'that', 'were', 'second', 'in', 'education', 'and', 'i', 'have', 'seen', 'soviet', 'education', 'and', 'ive', 'seen', 'ours', 'and', 'were', 'not', 'that', 'were', 'second', 'in', 'science', 'because', 'they', 'may', 'be', 'ahead', 'in', 'one', 'area', 'or', 'another', 'when', 'overall', 'were', 'way', 'ahead', 'of', 'the', 'soviet', 'union', 'and', 'all', 'other', 'countries', 'in', 'science', 'when', 'he', 'says', 'as', 'he', 'did', 'in', 'january', 'of', 'this', 'year', 'that', 'we', 'have', 'the', 'worst', 'slums', 'that', 'we', 'have', 'the', 'most', 'crowded', 'schools', 'when', 'he', 'says', 'that', 'seventeen', 'million', 'people', 'go', 'to', 'bed', 'hungry', 'every', 'night', 'when', 'he', 'makes', 'statements', 'like', 'this', 'what', 'does', 'this', 'do', 'to', 'american', 'prestige']\n",
      "longest sentence length:  149\n"
     ]
    }
   ],
   "source": [
    "df['Speech'] = df['Speech'].apply(preproc)\n",
    "valid = ['Claim', 'Premise', 'O']\n",
    "df = df.loc[(df['Component'].isin(valid))]\n",
    " \n",
    "classes = []\n",
    "\n",
    "for s in df.Component:\n",
    "    if s == 'O':\n",
    "        classes.append(0.0)\n",
    "    else:\n",
    "        classes.append(1.0)\n",
    "        \n",
    "df['Annotation'] = classes\n",
    "df.Annotation.value_counts()\n",
    "df = df[['Speech', 'Annotation', 'Set']]\n",
    "\n",
    "df_train = df[df['Set'] == 'TRAIN']\n",
    "df_val = df[df['Set'] == 'VALIDATION']\n",
    "df_test = df[df['Set'] == 'TEST']\n",
    "\n",
    "all_sentences = df.iloc[:, 0].tolist()\n",
    "all_sentences_train = df_train.iloc[:, 0].tolist()\n",
    "all_sentences_test = df_test.iloc[:, 0].tolist()\n",
    "\n",
    "all_labels_train = df_train.iloc[:, 1].tolist()\n",
    "all_labels_test = df_test.iloc[:, 1].tolist()\n",
    "\n",
    "all_sent_train_tokenized = []\n",
    "all_sent_test_tokenized = []\n",
    "all_sent_tokenized = []\n",
    "longest_word_len = []\n",
    "for i in range (len(all_sentences)):\n",
    "    all_sent_tokenized.append(word_tokenize(all_sentences[i]))\n",
    "    \n",
    "for i in range (len(all_sentences_train)):\n",
    "    all_sent_train_tokenized.append(word_tokenize(all_sentences_train[i]))\n",
    "    \n",
    "for i in range (len(all_sentences_test)):\n",
    "    all_sent_test_tokenized.append(word_tokenize(all_sentences_test[i]))\n",
    "    \n",
    "print(\"longest sentence: \", max(all_sent_tokenized,key=len))\n",
    "print(\"longest sentence length: \", len(max(all_sent_tokenized,key=len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-launch",
   "metadata": {},
   "source": [
    "**We import the fasttext embeddings and then represent each tokenized sentence as indices of the respective tokens in the embedding vocabulary. All sentences will be padded to match the length of the longest sentence in the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "several-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "FT = \"../../wiki-news-300d-1M.vec\"\n",
    "fasttext = KeyedVectors.load_word2vec_format(FT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "motivated-surname",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (14044, 149)\n",
      "Shape of data: (8455, 149)\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 149\n",
    "vocab = Counter()\n",
    "\n",
    "for sent in all_sent_tokenized:\n",
    "    vocab.update(sent)\n",
    "    \n",
    "unique_words = len(fasttext)\n",
    "word_index = {t[0]: i+1 for i,t in enumerate(vocab.most_common(unique_words))}\n",
    "\n",
    "sequences_train = [[word_index.get(t, 0) for t in sent] for sent in all_sent_train_tokenized]\n",
    "sequences_test = [[word_index.get(t, 0) for t in sent] for sent in all_sent_test_tokenized]\n",
    "\n",
    "data_train = pad_sequences(sequences_train, maxlen=max_seq_len, padding=\"pre\", truncating=\"post\")\n",
    "print('Shape of data:', data_train.shape)\n",
    "\n",
    "data_test = pad_sequences(sequences_test, maxlen=max_seq_len, padding=\"pre\", truncating=\"post\")\n",
    "print('Shape of data:', data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-majority",
   "metadata": {},
   "source": [
    "**An embedding matrix will be created as the input embedding layer in the LSTM network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "protected-holder",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = (np.random.rand(unique_words, 300) - 0.5) / 5.0\n",
    "for word, i in word_index.items():\n",
    "    if i >= unique_words:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = fasttext[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        pass  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-assault",
   "metadata": {},
   "source": [
    "**The bidirectional LSTM network is created with with an embedding layer whose weights are the pretrained fasttext embeddings and has a dimension of 300. We use 128 neurons for the bidirectional LSTM layer, sigmoid activation function for the output layer, binary_crossentropy for the loss function, and adam optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "broke-patient",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 300)         299998200 \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 256)              439296    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 300,437,753\n",
      "Trainable params: 439,553\n",
      "Non-trainable params: 299,998,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with tf.device('cpu:0'):\n",
    "  embedding_layer = Embedding(len(fasttext), 300, weights = [embedding_matrix] , trainable=False)\n",
    "  embedding_layer.build((len(fasttext), 300))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=False), input_shape=(300, 1)))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"Precision\",\"Recall\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-chambers",
   "metadata": {},
   "source": [
    "**Train and test the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "usual-catering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000012F595BB678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000012F595BB678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "439/439 [==============================] - 93s 200ms/step - loss: 0.4788 - precision: 0.8006 - recall: 0.9622\n",
      "Epoch 2/5\n",
      "439/439 [==============================] - 78s 179ms/step - loss: 0.4434 - precision: 0.8226 - recall: 0.9514\n",
      "Epoch 3/5\n",
      "439/439 [==============================] - 94s 214ms/step - loss: 0.4234 - precision: 0.8292 - recall: 0.9537\n",
      "Epoch 4/5\n",
      "439/439 [==============================] - 93s 213ms/step - loss: 0.4138 - precision: 0.8344 - recall: 0.9496\n",
      "Epoch 5/5\n",
      "439/439 [==============================] - 90s 206ms/step - loss: 0.4009 - precision: 0.8387 - recall: 0.9510\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12fafa9cf88>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = np.asarray(data_train)\n",
    "all_labels_train = np.asarray(all_labels_train)\n",
    "model.fit(x=data_train, y=all_labels_train, epochs=5,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "military-drama",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000013093D559D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000013093D559D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None      0.748     0.403     0.524      1880\n",
      "         Arg      0.849     0.961     0.902      6575\n",
      "\n",
      "    accuracy                          0.837      8455\n",
      "   macro avg      0.799     0.682     0.713      8455\n",
      "weighted avg      0.827     0.837     0.818      8455\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(data_test)\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i][0] > 0.5:\n",
    "        y_pred[i][0] = 1\n",
    "    else:\n",
    "        y_pred[i][0] = 0\n",
    "        \n",
    "target_names = ['None', 'Arg']\n",
    "print(classification_report(all_labels_test, y_pred, target_names=target_names, digits=3))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "accredited-onion",
   "metadata": {},
   "source": [
    "                precision   recall  f1-score   support\n",
    "\n",
    "        None      0.748     0.403     0.524      1880\n",
    "         Arg      0.849     0.961     0.902      6575\n",
    "\n",
    "    accuracy                          0.837      8455\n",
    "   macro avg      0.799     0.682     0.713      8455\n",
    "weighted avg      0.827     0.837     0.818      8455"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
